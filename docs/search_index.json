[["index.html", "STAT 101 IPS Lecture notes [WSMiP UŁ] Course description", " STAT 101 IPS Lecture notes [WSMiP UŁ] Michał Pierzgalski 2020-11-11 Course description Before you start reading, please look into the detailed syllabus: STAT101 "],["contact.html", "Contact", " Contact Email: michal.pierzgalski@uni.lodz.pl Personal website: https://pierzgal.github.io/michalpierzgalski/ Office hours (2020/2021) By appointment (via MS Teams); please, arrange the day and time via an official email address. IMPORTANT! Students, please contact me only via e-mail in the domain @uni.lodz.pl and @edu.uni.lodz.pl. In the email subject, please reference the course name, major, minor, etc. Students in contact with employees of the University of Lodz are required to use addresses in the domains of the University of Lodz and correspondence on the line student - teacher can only be conducted via these e-mail addresses. I reply to emails from students only on Fridays and Mondays or during my office hours. "],["introduction-lecture-1.html", "1 Introduction [Lecture 1]", " 1 Introduction [Lecture 1] Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. "],["motivation-two-reasons-why-you-should-learn-statistics-and-data-analysis-methods.html", "1.1 Motivation - two reasons why you should learn statistics and data analysis methods?", " 1.1 Motivation - two reasons why you should learn statistics and data analysis methods? In social research (political science, sociology, psychology or economics), statistical methods are used to find answers to research questions or to test research hypotheses. Statistics is a basic research tool for social scientists. Statistics and related disciplines such as Data Science or Machine Learning allow us to better understand modern technologies (such as Computational photography, Google’s Spam Filter, Self-driving cars) that not only make our lives easier, but also have an impact on politics and current society (e.g. the so-called microtargeting 1 - it “is a form of targeting that uses recent technological developments to gather large amounts of online data. The data from people’s digital footprint’s analyzed to create and convey messages that reflect an individual’s preferences and personality” (Wikipedia). Microtargeting (via social media like Facebook) can be used e.g. in election campaigns. Zob. np. Mikrotargetowanie i datakracja: https://wszystkoconajwazniejsze.pl/jan-sliwa-mikrotargetowanie-i-datakracja/↩︎ "],["review-of-some-mathematical-concepts-used-in-statistics-and-data-science.html", "1.2 Review of some mathematical concepts used in statistics and data science", " 1.2 Review of some mathematical concepts used in statistics and data science 1.2.1 Algebra review - some basic rules 1.2.1.1 Identity There are equalities of two different types: equations and identities. An equality \\(=\\) is not the same as identity \\(\\equiv\\). The \\(\\equiv\\) symbol originally meant “is identically equal to”. An identity is a type of equality which is true for all values, e.g.: \\((a+b)^2 \\equiv a^2+2ab+b^2.\\) 1.2.1.2 Differences - Delta notation The Greek letter \\(\\Delta\\) (capital delta) is the symbol used to indicate the difference in a measured quantity, usually at two different times. 1.2.1.3 Summation - Sigma notation In physics there are often contexts in which it’s necessary to add several quantities. A useful abbreviation for representing such a sum is the Greek letter \\(\\Sigma\\) (capital sigma). Suppose we wish to add a set of five numbers represented by \\(x1, x2, x3, x4, x5\\). In the abbreviated notation, we would write the sum as \\(x1 + x2 + x3 + x4 + x5 = \\sum_{i=1}^{5} x_i\\) where the subscript i on x represents any one of the numbers in the set. For example, if there are five masses in a system, \\(m1, m2, m3, m4, m5\\), the total mass of the system \\(M = m1 + m2 + m3 + m4 + m5\\) could be expressed as \\(M = \\sum_{i=1}^{5} m_i\\) 1.2.1.4 Absolute value The magnitude of a quantity x, written x, is simply the absolute value of that quantity. The sign of x is always positive, regardless of the sign of x. For example, if x = -5, then |x| = 5; if x = 8, then |x| = 8. 1.2.1.5 Basic algebraic operations When algebraic operations are performed, the laws of arithmetic apply. Symbols such as x, y, and z are usually used to represent unspecified quantities, called the unknowns (or variables). First, consider the equation \\(8x = 32\\) If we wish to solve for x, we can divide (or multiply) each side of the equation by the same factor without destroying the equality. In this case, if we divide both sides by 8, we have \\(\\frac{8x}{8} = \\frac{32}{8}\\) \\(x = 4\\) Also, consider the equation \\(x + 2 = 8\\) In this type of expression, we can add or subtract the same quantity from each side. If we subtract 2 from each side, we have \\(x + 2 - 2 = 8 - 2\\) \\(x = 6\\) Now, consider the equation \\(\\frac{x}{5} = 9\\) If we multiply each side by 5, \\((\\frac{x}{5}) \\times 5 = 9 \\times 5\\) \\(x = 45\\) In all cases, whatever operation is performed on the left side of the equality must also be performed on the right side. 1.2.1.6 Some useful arithmetic rules Należy przypomnieć następujące zasady mnożenia, dzielenia, dodawania i odejmowania ułamków, gdzie a, b, c i d to cztery liczby: Rule Example Multiplying \\((\\frac{a}{b})(\\frac{c}{d}) = \\frac{ac}{bd}\\) Dividing \\((\\frac{a/b}{c/d}) = \\frac{ad}{bc}\\) Adding \\((\\frac{a}{b}) \\pm (\\frac{c}{d}) = \\frac{ad \\pm bc}{bd}\\) 1.2.1.7 Powers The following rules apply: \\(x^nx^m = x^{n+m}\\) \\(\\frac{x^n}{x^m} = x^{n-m}\\) \\(x^{\\frac{1}{n}} = \\sqrt[n]{x}\\) \\((x^n)^m = x^{nm}\\) 1.2.1.8 Logarithms Suppose a quantity x is expressed as a power of some quantity a: \\(x = a^y\\) The number a is called the base number. The logarithm of x with respect to the base a is equal to the exponent to which the base must be raised to satisfy the expression \\(x = a^y\\): \\(y = \\log_a x\\) In practice, the two bases most often used are base 10, called the common logarithm base, and base \\(e = 2.718 282\\), called Euler’s constant or the natural logarithm base. For any base: \\(\\log{(ab)} = \\log a + \\log b\\) \\(\\log{a/b} = \\log a + \\log b\\) \\(\\log{a^n} = n \\log a\\) Also, \\(\\ln e\\) \\(\\ln e^a = a\\) \\(\\ln(1/a) = -\\ln a\\) 1.2.1.9 Factoring Some useful formulas for factoring an equation are the following: \\(ax + ay + az = a(x + y + z)\\) \\(a^2 + 2ab + b^2 = (a + b)^2\\) - so called “perfect square” \\(a^2 + b^2 = (a + b)(a - b)\\) 1.2.1.10 Linear equations A linear equation has the general form \\(y = mx + b\\) where m and b are constants. This equation is referred to as linear because the graph of y versus x is a straight line. The constant b, called the y-intercept, represents the value of y at which the straight line intersects the y axis. The constant m is equal to the slope of the straight line. If any two points on the straight line are specified by the coordinates (x1, y1) and (x2, y2), the slope of the straight line can be expressed as \\(Slope = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{\\Delta{y}}{\\Delta{x}}\\) Note that m and b can have either positive or negative values. If \\(m &gt; 0\\), the straight line has a positive slope. If \\(m &lt; 0\\), the straight line has a negative slope. Solving Simultaneous Linear Equations Consider the equation \\(3x + 5y = 15\\), which has two unknowns, x and y. Such an equation does not have a unique solution. For example, \\((x = 0, y = 3)\\), \\((x = 5, y = 0)\\), and (\\(x = 2, y = \\frac{9}{5}\\)) are all solutions to this equation. If a problem has two unknowns (variables), a unique solution is possible only if we have two equations. In general, if a problem has n unknowns, its solution requires n equations. To solve two simultaneous equations involving two unknowns, x and y, we solve one of the equations for x in terms of y and substitute this expression into the other equation. 1.2.2 Functions A function from a set A to another set B is an assignment of some element of B to each element in A. A function is a rule that assigns each input exactly one output. We call the output the image of the input. The set of all inputs for a function is called the domain. The set of all allowable outputs is called the codomain. In mathematics, a function is a binary relation between two sets that associates every element of the first set to exactly one element of the second set. Figure 1.1. Function2 Figure 1.2. Function3 A mathematical model is an abstract concept through which we use mathematical language and notation to describe a phenomenon in the world around us. Models describe our beliefs about how the world functions. In mathematical modelling, we translate those beliefs into the language of mathematics. A function can serve as a simple kind of mathematical model. Remember that a function is just a rule, f, that expresses the dependency of one variable quantity, y, on another variable quantity, x. We can think of the rule (given as a graph, a formula, or a table of values) as a representation of some natural cause and effect relationship – if x, then y – between the two variable quantities. For a mathematical model, we often seek an algebraic formula that captures observed behavior accurately and can be used to predict behavior not yet observed. 1.2.3 Sets Sets are denoted using curly braces \\(\\{\\}\\). e.g. \\(A = \\{0, 1\\}\\) 1.2.3.1 Set Definitions A set is a well-defined collection of objects. Each object in a set is called an element of the set. Two sets are equal if they have exactly the same elements in them. A set that contains no elements is called a null set or an empty set. If every element in Set A is also in Set B, then Set A is a subset of Set B. Membership: If \\(a\\) is a member of a set A, we write \\(a \\in A\\) Sets of numbers: set of natural numbers \\(N = \\{0, 1, 2, 3, ... \\}\\), set of positive natural numbers \\(N^+ = \\{1, 2, 3, ... \\}\\), a set of integers \\(Z = \\{..., -2, -1, 0, 1, 2, 3, ... \\}\\), a set of rational numbers \\(Q\\), i.e. all numbers that can be represented as \\(\\frac{p}{q}\\) for \\(q \\neq 0\\), set of irrational numbers \\(NQ\\), i.e. all numbers that have infinite and non-periodic decimal expansion, e.g. number \\(\\pi \\approx 3,141592...\\), or a number indicated by a letter \\(e \\approx 2,718 ...\\), set of real numbers \\(R = Q + NQ\\). \\(R\\) can be expressed as an interval \\((-\\infty, \\infty)\\). An interval is a set of real numbers with the property that any number that lies between two numbers in the set is also included in the set. The interval of numbers between a and b, including a and b, is often denoted \\([a, b]\\). The two numbers are called the endpoints of the interval. A singleton is a set with exactly one element. CAUTION: Be sure you understand the difference between the outcome -8 and the event {−8}, which is the set consisting of the single outcome −8. The cardinality (or size) of a collection or set \\(A\\), denoted \\(|A|\\), is the number of elements of the collection. This number may be finite or infinite. A finite set is a set that has a finite number of elements. In other words, it is either an empty set, a singleton, or a set whose elements can be listed in the form \\({a1, a2, . . . , an}\\) for some \\(n \\in N\\). A set that is not finite is called infinite. These sets have more than \\(n\\) elements for any integer \\(n\\). Whether finite or infinite, the elements of a countable set can always be counted one at a time and, although the counting may never finish, every element of the set is associated with a natural number. Countable sets form the foundation of a branch of mathematics called discrete mathematics. A set that is not countable is called uncountable set (or uncountably infinite set). It contains too many elements to be countable, e.g. an interval with positive length: \\([0, 1]\\). Table 1.1. The terminology of set theory and probability theory Set Theory Probability Theory Set Event Universal set Sample space Element Outcome Table 1.2. Probability event language - Event language \\(A\\) A occurs \\(A^c\\) A does not occur \\(A \\cup B\\) Either A or B occur \\(A \\cap B\\) Both A and B occur The difference between countable and uncountable sets is important for statistics and probability. Because of the mathematics required to determine probabilities, probabilistic methods are divided into two distinct types, discrete and continuous. A discrete approach is used when the number of experimental outcomes is finite (or infinite but countable). A continuous approach is used when the outcomes are continuous (and therefore infinite). It will be important to keep in mind which case is under consideration since otherwise, certain paradoxes may result. 1.2.4 Three basic concepts of calculus Limit of function; Derivative; Integral. 1.2.5 Limits In mathematics, a limit is the value that a function (or sequence) “approaches” as the input (or index) “approaches” some value. Limits are essential to calculus and mathematical analysis, and are used to define e.g. derivatives, and integrals. For \\(f(x)\\), we have \\[\\lim_{x \\rightarrow c}f(x)=L\\] For example, \\[\\lim_{x \\rightarrow 0}\\frac{1}{x}=\\infty\\] or, \\[\\lim_{x \\rightarrow \\infty}\\frac{1}{x}=0\\] In formulas, a limit of a function is usually written as \\[\\lim_{x \\to c}f(x)=L\\] and is read as “the limit of f of x as x approaches c equals L”. The fact that a function f approaches the limit L as x approaches c is usually denoted by a right arrow \\(\\to\\), as in: \\[f(x)\\to L{\\text{ as }}x\\to c\\] which reads \\(f(x)\\) tends to \\(L\\) as \\(x\\) tends to \\(c\\). Figure 1.3. Limits4 1.2.6 Derivative First, a function must be specified that relates one variable to another (e.g., a coordinate as a function of time). Suppose one of the variables is called y (the dependent variable), and the other x (the independent variable). We might have a function relationship such as \\(y(x) = ax^2 + bx + c\\) If a, b, and c are specified constants, y can be calculated for any value of x. We usually deal with continuous functions, that is, those for which y varies “smoothly” with x. The derivative of y with respect to x is defined as the limit as \\(\\Delta x\\) approaches zero of the slopes of chords drawn between two points on the y versus x curve. Mathematically, we write this definition as \\[\\frac{dy}{dx} = \\lim_{\\Delta x \\to\\ 0} \\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to\\ 0} \\frac{y(x + \\Delta x) - y(x)}{\\Delta x}\\] where \\(\\Delta y = y_2 - y_1\\) and \\(\\Delta x = x_2 - x_1\\). Note that \\(dy/dx\\) does not mean \\(dy\\) divided by \\(dx\\), but rather is simply a notation of the limiting process of the derivative (differentiation operator). A useful expression to remember when \\(y = ax^n\\), where a is a constant and n is any positive or negative number (integer or fraction), is \\(\\frac{dy}{dx} = nax^{n-1}\\) Figure 1.4. Derivative - visual representation5 1.2.7 Integral We think of integration as the inverse of differentiation. As an example, consider the expression 1.1 \\(f(x) = \\frac{dy}{dx} = 3ax^2 + b\\) which was the result of differentiating the function 1.2 \\(y(x) = ax^3 = bx + c\\) We can write equation 1.1 as \\(dy = f(x)dx = (3ax^2 + b)dx\\) and obtain \\(y(x)\\) by “summing” over all values of x. Mathematically, we write this inverse operation as \\[y(x) = \\int f(x) dx\\] \\[ y(x) = \\int (3az^2 + b)dx = ax^3 + bx + c\\] where c is a constant of the integration. This type of integral is called an indefinite integral because its value depends on the choice of c. 1.2.8 Integral as an area - intuitive explanation For a general continuous function f(x), the integral can be described as the area under the curve bounded by f(x) and the x axis, between two specified values of x, say, \\(x1\\) and \\(x2\\), as in the Figure 1.5. The area of the blue element in Figure 1.5 is approximately \\(f(x_i) \\Delta x_i\\). If we sum all these area elements between \\(x1\\) and \\(x2\\) and take the limit of this sum as \\(\\Delta x_i \\to 0\\), we obtain the true area under the curve bounded by f(x) and the x axis, between the limits \\(x1\\) and \\(x2\\): \\[Area = \\lim_{\\Delta x_i \\to\\ 0} \\sum_i f(x_i)\\Delta x_i = \\int_{x_1}^{x_2} f(x)dx\\] Integrals of this type are called definite integrals. Figure 1.5. Definite integral as an area under a curve6 Figure 1.6. Definite integral as an area under a curve7 Figure 1.7. Definite integral as an area under a curve8 1.2.8.1 Summary Figure 1.8. Summary of basic calulus concepts9 1.2.9 Probability Probability is the bedrock of statistics and data science. Later, we will review the basics of probability theory in detail. Retrieved from: Sets and Functions. https://mathigon.org/course/sets-and-functions/function-properties↩︎ Retrieved from: Loup Vaillant. http://loup-vaillant.fr/tutorials/from-imperative-to-functional↩︎ Retrieved from: Math24. https://www.math24.net/definition-limit-function/↩︎ Retrieved from: Wikimedia. https://commons.wikimedia.org/wiki/File:Derivative_-_geometric_meaning.svg↩︎ Retrieved from: Serway, R. A., &amp; Jewett, J. W. (2018). Physics for scientists and engineers with modern physics. Cengage learning.↩︎ Retrieved from: Active Calculus. https://activecalculus.org/single/sec-4-3-definite-integral.html↩︎ Retrieved from: Active Calculus. https://activecalculus.org/single/sec-4-3-definite-integral.html↩︎ Retrieved from: Derivatives on Unequally Spaced Grids. http://cococubed.asu.edu/code_pages/fdcoef.shtml↩︎ "],["statistics-an-introduction-lecture-2.html", "2 Statistics - an introduction [Lecture 2]", " 2 Statistics - an introduction [Lecture 2] Statistics is the science of learning from data. Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys or experiments. "],["essential-concepts.html", "2.1 Essential concepts", " 2.1 Essential concepts Data refers to a set of values, which are usually organized by variables (what is being measured) and observational units (members of the sample/population). An example of data is a data matrix in a spreadsheet program, such as Google Sheets. A collection of observations on one or more variables. Variable: A characteristic whose value may change from one observation to another. When we want to talk about the influence of a factor on a characteristic of interest, we identify the factor (a variable) as the independent variable (often called a predictor or explanatory variable) and the affected variable as the dependent variable (often called the response variable). Population: The entire collection of individuals or objects about which information is desired is called the population of interest. Sample: A sample is a subset of the population, selected for study. 2.1.1 A parameter and a statistic A parameter is a value, usually a numerical value, that describes a population. A parameter is usually derived from measurements of the individuals in the population. A statistic is a value, usually a numerical value, that describes a sample. A statistic is usually derived from measurements of the individuals in the sample. "],["constructs-and-operational-definitionslecture-1.html", "2.2 Constructs and operational definitions10", " 2.2 Constructs and operational definitions10 “Some variables, such as height, weight, and eye color are well-defined, concrete entities that can be observed and measured directly. On the other hand, many variables studied by behavioral scientists are internal characteristics that people use to help describe and explain behavior. For example, we say that a student does well in school because he or she is intelligent. Or we say that someone is anxious in social situations, or that someone seems to be hungry. Variables like intelligence, anxiety, and hunger are called constructs, and because they are intangible and cannot be directly observed, they are often called hypothetical constructs.” Constructs are internal attributes or characteristics that cannot be directly observed but are useful for describing and explaining behavior. An operational definition identifies a measurement procedure (a set of operations) for measuring an external behavior and uses the resulting measurements as a definition and a measurement of a hypothetical construct. Note that an operational definition has two components. First, it describes a set of operations for measuring a construct. Second, it defines the construct in terms of the resulting measurements. 2.2.1 Conceptualisation and operationalisation Conceptualization is the mental process by which fuzzy and imprecise constructs (concepts) and their constituent components are defined in concrete and precise terms. For example, the process of understanding what is included and what is excluded in the concept of e.g. “prejudice” is the conceptualization process. The conceptualization process is all the more important because of the imprecision, vagueness, and ambiguity of many social science constructs. While defining constructs such as prejudice or compassion, we must understand that sometimes, these constructs are not real or can exist independently, but are simply imaginary creations in our mind. Once a theoretical construct is defined, exactly how do we measure it? Operationalization refers to the process of developing indicators or items for measuring these constructs. For instance, if an unobservable theoretical construct such as socioeconomic status is defined as the level of family income, it can be operationalized using an indicator that asks respondents the question: what is your annual family income? Given the high level of subjectivity and imprecision inherent in social science constructs, we tend to measure most of those constructs (except a few demographic constructs such as age, gender, education, and income) using multiple indicators. This process allows us to examine the closeness amongst these indicators as an assessment of their accuracy (reliability). Retrieved from: Gravetter, F.J., Wallnau, L.B., Forzano, L.A.B. and Witnauer, J.E., 2020. *Essentials of statistics for the behavioral sciences*. Cengage Learning.↩︎ "],["data-collection.html", "2.3 Data collection", " 2.3 Data collection Generally, you can obtain data in three different ways: 1. From a published source; 2. From a designed experiment; 3. From an observational study (e.g., a survey). Moreover, you can obtain data using a simulation: A simulation is the use of a mathematical or physical model to reproduce the conditions of a situation or process. Collecting data often involves the use of computers. Simulations allow you to study situations that are impractical or even dangerous to create in real life, and often they save time and money. For instance, automobile manufacturers use simulations with dummies to study the effects of crashes on humans. Throughout this course, you will have the opportunity to use applets that simulate statistical processes on a computer. "],["sampling-techniqueslecture-2.html", "2.4 Sampling techniques11", " 2.4 Sampling techniques11 A census is a count or measure of an entire population. Taking a census provides complete information, but it is often costly and difficult to perform. A sampling is a count or measure of part of a population, and is more commonly used in statistical studies. To collect unbiased data, a researcher must ensure that the sample is representative of the population. Appropriate sampling techniques must be used to ensure that inferences about the population are valid. Remember that when a study is done with faulty data, the results are questionable. Even with the best methods of sampling, a sampling error may occur. A sampling error is the difference between the results of a sample and those of the population. When you learn about inferential statistics, you will learn techniques of controlling sampling errors. A random sample is one in which every member of the population has an equal chance of being selected. A simple random sample is a sample in which every possible sample of the same size has the same chance of being selected. One way to collect a simple random sample is to assign a different number to each member of the population and then use a random number table Larson, R. and Farber, B., 2019. Elementary statistics. Pearson Education Canada.↩︎ "],["data-distribution.html", "2.5 Data distribution", " 2.5 Data distribution The first step in analyzing data collected on a variable is to look at the observed values by using graphs and numerical summaries. The goal is to describe key features of the distribution of a variable. The most crucial step to exploratory data analysis is estimating the distribution of a variable. The distribution of a variable describes how the observations fall (are distributed) across the range of possible values. The distribution of a data set is a table, graph, or formula that provides the values of the observations and how often they occur. An important aspect of the distribution of a quantitative data set is its shape. Indeed, the shape of a distribution frequently plays a role in determining the appropriate method of statistical analysis. To identify the shape of a distribution, the best approach usually is to use a smooth curve that approximates the overall shape. A frequency distribution is a collection of observations produced by sorting observations into classes and showing their frequency of occurrence in each class: The (frequency) distribution of data - name the possible observations (numbers, categories) and tell how frequently each occurs. A frequency distribution is a summary table in which the data are arranged into numerically ordered classes or intervals. A relative frequency distribution is obtained by dividing the frequency in each class by the total number of values. From this a percentage distribution can be obtained by multiplying each relative frequency by 100%. Frequency distribution for categorical data: A table that displays the possible categories along with the associated frequencies and/or relative frequencies. Frequency: The frequency for a particular category is the number of times the category appears in the data set. The relative frequency for a particular category is the proportion of the observations that belong to that category. "],["basic-stages-of-social-researchlecture-3.html", "2.6 Basic stages of social research12", " 2.6 Basic stages of social research12 Systematically testing our ideas about the nature of social reality often demands carefully planned and executed research in which the following occur: The problem to be studied is reduced to a testable hypothesis (for example, “One-parent families generate more delinquency than two-parent families”). An appropriate set of instruments is developed (for example, a questionnaire or an interview schedule). The data are collected (that is, the researcher might go into the field and conduct a poll or a survey). The data are analyzed for their bearing on the initial hypotheses. Results of the analysis are interpreted and communicated to an audience (for example, by means of a lecture, journal article, or press release). Retrieved from: Fox, J.A., Levin, J. and Forde, D.R., 2017. Elementary Statistics in Social Research.↩︎ "],["statistical-study-essential-steps.html", "2.7 Statistical study - essential steps", " 2.7 Statistical study - essential steps Identify the variable(s) of interest (the focus) and the population of the study. Develop a detailed plan for collecting data. If you use a sample, make sure the sample is representative of the population. Collect the data. Describe the data, using descriptive statistics techniques. Interpret the data and make decisions about the population using inferential statistics. Identify any possible errors. "],["experimental-and-observational-studieslecture-4.html", "2.8 Experimental and observational studies13", " 2.8 Experimental and observational studies13 A statistical study can usually be categorized as an observational study or an experiment. In an observational study, a researcher does not influence the responses. In an experiment, a researcher deliberately applies a treatment before observing the responses. Here is a brief summary of these types of studies. In an observational study, a researcher observes and measures characteristics of interest of part of a population but does not change existing conditions. For instance, an observational study was performed in which researchers observed and recorded the mouthing behavior on nonfood objects of children up to three years old. (Source: Pediatrics Magazine) In performing an experiment, a treatment is applied to part of a population, called a treatment group, and responses are observed. Another part of the population may be used as a control group, in which no treatment is applied. (The subjects in the treatment and control groups are called experimental units.) In many cases, subjects in the control group are given a placebo, which is a harmless, fake treatment, that is made to look like the real treatment. The responses of the treatment group and control group can then be compared and studied. In most cases, it is a good idea to use the same number of subjects for each group. For instance, an experiment was performed in which diabetics took cinnamon extract daily while a control group took none. After 40 days, the diabetics who took the cinnamon reduced their risk of heart disease while the control group experienced no change. (Source: Diabetes Care) Retrieved from: Larson, R., Farber, E. and Farber, E., 2009. Elementary statistics: Picturing the world. Pearson Prentice Hall.↩︎ "],["statistical-research-design-an-example-of-a-simple-experimentlecture-5.html", "2.9 Statistical research design - an example of a simple experiment14", " 2.9 Statistical research design - an example of a simple experiment14 Experimental research design - overview Gravetter, F.J., Wallnau, L.B., Forzano, L.A.B. and Witnauer, J.E., 2020. Essentials of statistics for the behavioral sciences. Cengage Learning.↩︎ "],["descriptive-and-inferential-statistics.html", "2.10 Descriptive and inferential statistics", " 2.10 Descriptive and inferential statistics Descriptive statistics: The branch of statistics that includes methods for organizing and summarizing data. Statistical tools and ideas help us examine data to describe their main features. This examination is called exploratory data analysis. Here are two basic strategies that help us organize our exploration of a set of data: Begin by examining each variable by itself. Then move on to study the relationships among the variables. Begin with a graph or graphs. Then add numerical summaries of specific aspects of the data. Inferential statistics: The branch of statistics that involves generalizing from a sample to the population from which the sample was selected and assessing the reliability of such generalizations. "],["random-samplinglecture-6.html", "2.11 Random sampling15", " 2.11 Random sampling15 A random sample is one that is selected without bias. In selecting a sample, the importance of randomness cannot be overemphasized. In statistical thinking we expect a random sample to share approximately the same properties as the population. Also, the larger the sample size, the more closely the sample properties approximate those of the population. For example, to estimate the average height of adult males, it would be highly biased to select a sample from among NBA players. In general, non-random samples are generated when there is bias in the selection process. Such samples are useless for statistical purposes, because the sample is not representative of the population. A simple random sample is one in which every individual in the population has the same probability of being selected. To satisfy this requirement, the sampling method that is used must be free of bias with respect to the property being measured. The “lottery method” of selection appears to produce a simple random sample (see the margin). Computer-generated random numbers (also called pseudo-random numbers) can be used in selecting random samples: We first assign a number to each individual in the population and then use a computer random number generator to select from the assigned numbers. The following are common types of sampling bias. 2.11.1 Sampling errors and systematic bias In statistics, sampling errors are incurred when the statistical characteristics of a population are estimated from a subset, or sample, of that population. Since the sample does not include all members of the population, statistics on the sample, such as means and quartiles, generally differ from the characteristics of the entire population, which are known as parameters. For example, if one measures the height of a thousand individuals from a country of one million, the average height of the thousand is typically not the same as the average height of all one million people in the country. Since sampling is typically done to determine the characteristics of a whole population, the difference between the sample and population values is considered an error. Systematic error is predictable and typically constant or proportional to the true value. If the cause of the systematic error can be identified, then it usually can be eliminated. Systematic errors are caused by imperfect calibration of measurement instruments or imperfect methods of observation, or interference of the environment with the measurement process, and always affect the results of an experiment in a predictable direction. 2.11.2 Various types of bias in statistical analysis Undercoverage bias (or exclusion bias), in which part of the population is excluded from the sampling process. Response bias, in which the wording of a questionnaire is not neutral but rather suggests or provokes a particular response. Nonresponse bias, in which individuals with a common characteristic are unwilling (or neglect) to respond to a questionnaire. (Notice that this is not the opposite of response bias.) Self-selection bias (or voluntary response bias), in which individuals select themselves (or volunteer) for the sample. Many of these biases are a result of convenience sampling, in which individuals are sampled only because they are nearby or easily accessible. Selection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. The phrase “selection bias” most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may be false. In practice, it is difficult to ensure the selection of a random sample. Several methods have been developed for sampling in particular situations. Some of the most common are as follows. Systematic Sampling: In systematic sampling, a sample is chosen systematically from a list. For example, we may pick every 100th name in a telephone book. Stratified Sampling: In stratified sampling, the population is first divided into nonoverlapping groups (or strata), and then the sample is chosen proportionally from each group. For example, to pick a sample of registered voters, we may want to stratify the population into groups—white, African American, Hispanic, and other—and then randomly pick registered voters, choosing from each group a number proportional to the size of the group in the population. Cluster Sampling: In cluster sampling, the population is divided into groups (or clusters) and then a random sample of clusters is selected. For example, to survey apartment dwellers in Los Angeles, we would first randomly select a collection of apartment buildings (the clusters) and then interview every resident in the selected buildings. This type of sampling reduces the enormous time and cost for the pollster in traveling from apartment to apartment. Retrieved from: Stewart, J., Redlin, L. and Watson, S., 2013. Precalculus: Mathematics for calculus. Cengage Learning.↩︎ "],["experimental-designlecture-7.html", "2.12 Experimental design16", " 2.12 Experimental design16 In observational studies, the researcher has no control over the factors affecting the property being studied—the researcher is merely an observer. Extraneous or unintended variables that systematically affect the property being studied are called confounding variables (or confounding factors or lurking variables). Such variables are said to confound (or mix up) the results of the study. The following examples show how this can happen. To eliminate or vastly reduce the effects of confounding variables, researchers often conduct experiments so that such variables can be controlled. In an experimental study, two groups are selected: a treatment group (in which individuals are given a treatment) and a control group (in which individuals are not given the treatment). The individuals in the experiment are called subjects (or experimental units). The goal is to measure the response of the subjects to the treatment—that is, whether or not the treatment has an effect. The next step is to make sure that the two groups are as similar as possible except for the treatment. If the two groups are alike except for the treatment, then any statistical difference in response between the groups can be confidently attributed to the treatment. Here are some typical experimental designs. 1. Completely Randomized Design: The effects of unknown variables that may confound the experiment can be reduced or eliminated by randomization, that is, by assigning individuals randomly to the treatment or control groups. Randomization ensures that the effects of any confounding variables are equally likely to occur in either group. So any difference between the two groups in the response to the treatment can be attributed to the treatment. 2. Randomized Block Design: Variables that are known (prior to the experiment) to affect the response can be controlled by blocking. The participants are arranged into blocks (or groups) consisting of subjects with similar characteristics, and then treatment and control groups are randomly selected within each block. For example, if sex is a known source of variability in response, then a male block and a female block are formed. Treatment and control subjects are then randomly selected from the male block and from the female block. This design specifically “controls” for a known confounding factor by ensuring equal participation of individuals from each block in the treatment and control groups. 3. Matched Pair Design: In this design, the subjects are matched in pairs based on variables that may affect the response to the treatment. For example, the subjects of the study may be matched in pairs based on age and sex (a young male with a young male, a senior female with a senior female, and so on). Then an individual from each pair is randomly assigned to the treatment group, and the other is assigned to the control group. In this example, the treatment and control groups have equal participation with respect to two possible confounding variables (age and sex). A common confounding factor is the placebo effect, in which patients who think they are receiving a medication report an improvement (perceived or actual), even though the “treatment” they received was a placebo—a simulated or false treatment (sometimes called a “sugar pill”). To control the placebo effect, a researcher may use single-blinding, a method in which subjects don’t know whether they are in the treatment or control group, or double-blinding, in which the researchers are also not privy to this information during the course of the experiment. Replication, the repetition of the experiment, can also reinforce the reliability of the results. Retrieved from: Stewart, J., Redlin, L. and Watson, S., 2013. Precalculus: Mathematics for calculus. Cengage Learning.↩︎ "],["relevant-modern-application-of-statistical-theory-machine-learning-the-conceptual-introduction.html", "2.13 Relevant modern application of statistical theory: Machine learning - the conceptual introduction", " 2.13 Relevant modern application of statistical theory: Machine learning - the conceptual introduction Definitions Machine learning (ML) is a sub-field of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning algorithms build a mathematical model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to do so. Basic concepts used in Machine Learning: Algorithm: A Machine Learning algorithm is a set of rules and statistical techniques used to learn patterns from data and draw significant information from it. It is the logic behind a Machine Learning model. An example of a Machine Learning algorithm is the Linear Regression algorithm. Model: A model is the main component of Machine Learning. A model is trained by using a Machine Learning Algorithm. An algorithm maps all the decisions that a model is supposed to take based on the given input, in order to get the correct output. Predictor Variable: It is a feature(s) of the data that can be used to predict the output. Response Variable: It is the feature or the output variable that needs to be predicted by using the predictor variable(s). Training Data: The Machine Learning model is built using the training data. The training data helps the model to identify key trends and patterns essential to predict the output. Testing Data: After the model is trained, it must be tested to evaluate how accurately it can predict an outcome. This is done by the testing data set. 2.13.1 Machine learning approaches [From Wikipedia.org:] Supervised learning: The computer is presented with example inputs and their desired outputs, given by a “teacher”, and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that’s analogous to rewards, which it tries to maximize. “For the process of learning (model fitting) we need to have available some observations or data (also known as samples or examples) in order to explore potential underlying patterns, hidden in our data. These learned patterns are nothing more that some functions or decision boundaries.” 2.13.2 Make predictions or decisions using ML In science and technology, predictions or decisions are made using formal models. A very simple mathematical deterministic model can be: \\[F = \\frac{9}{5}(C) + 32\\] which you could use to convert degrees Celsius to degrees Fahrenheit. For example: \\[F = \\frac{9}{5}(39) + 32 = 70.2 + 32 = 102.2\\] You might be familiar with the formula for a line using the slope and y-intercept (recall the equation of linear function - \\(y = ax + b\\)): \\(y\\) = predicted value (dependent variable); \\(a\\) = slope, i.e. \\(\\frac{\\text{change in }y}{\\text{change in }x}\\); \\(b\\) = intercept with \\(x-\\)axis. However, in statistics, we rarely deal with deterministic models which will produce perfect prediction. We have stochastic models which will have unexplained (“random”) error. If you have an equation that tries to predict what the adult height of a newborn baby boy based on the height of his father. You will certainly not get a perfect prediction for many reasons. A mathematical version of a simple linear regression model, which is stochastic, would be: \\[y = \\beta_0 + x_1\\beta_1 + ... + x_k\\beta_k + \\epsilon\\] This denotes that we believe that there is a linear relationship between predictor variables \\(x_k\\) and response variable \\(y\\) with some unexplained error \\(\\epsilon\\). 2.13.3 Machine learning approaches - visual guide The following figures give general idea on how machine learning works17: Supervised learning Unsupervised learning Reinforcement learning 2.13.3.1 Supervised learning - example18 We seek to teach a computer to predict housing prices. We begin with giving our computer the prices of other houses in the area, as well as information about each house (e.g. the size, number of bedrooms, number of floors). This data are called the training set. Also, the data provided for any one of the given houses is called a training example - this is denoted by \\(x^{(i)}\\), meaning data pertaining to house (i), while \\(x^{(2)}\\) = training example (2). Each distinct bit of information contained within a training example is called a feature. In data representing housing attributes, the size of the house is a feature, as would the number of stories and the number of bedrooms. Every training example contains the same features in the same order. Features are denoted with \\(j\\). That is: \\(x^{(i)}_j\\) is a feature \\(j\\) for an example \\(i\\). Linear regression is one of the most basic machine learning models. Symbolically, this is a common representation of the linear regression model: \\[h_{\\theta}(x) = \\theta_0 + \\theta_1 x\\] \\(h_{\\theta}(x)\\) represents the hypothesis function (hypothesis). It says that the outputted value varies based on the input \\(x\\) That is to say, based on the value \\(x\\) between parentheses, the prediction is made, meaning that \\(h_{\\theta}(x^{(i)})\\) symbolizes the prediction made by the hypothesis on example \\(x^{(i)}\\). The \\(\\theta\\) symbols are known as parameters - can be positive or negative numbers. Also, theta zero is called a bias unit. “Let’s go back to our housing prices example. Let’s pretend we’re using training example 1. Our example will use only one feature for this explanation: the size of the house in square feet. This will be represented by \\(x^{(1)}_1\\), meaning feature 1 of training example 1, and will be equal to 2,500 (square feet).” \"Let’s say our hypothesis believes that each square foot adds $50 of value to the home and that housing prices start at $200,000 in the area. \\(\\theta_0\\) would then have a value of 200,000 and \\(\\theta_1\\) would then be 50. Let’s check out why that is. We know that a prediction is formed by summing the bias unit with the product of features and their respective parameters. When we multiply our \\(x^{(1)}_1\\) (2,500 square feet) by our \\(\\theta_1\\) (the associated parameter of $50/square feet), we get a value of $125,000. Through this multiplication, for every square foot (\\(x^{(1)}_1\\)), $50 dollars of value (\\(\\theta_1\\)) is added. We add that product to our bias unit of $200,000 (\\(\\theta_0\\) representing the average local starting housing price) to get a final output value of $325,000!\" Retrieved from: Loukas, S. What is Machine Learning: Supervised, Unsupervised, Semi-Supervised and Reinforcement learning methods. https://towardsdatascience.com/what-is-machine-learning-a-short-note-on-supervised-unsupervised-semi-supervised-and-aed1573ae9bb↩︎ Based on: Nolan, F. A Comprehensive Introductory Guide to Supervised Learning for the Non-Mathematician. https://towardsdatascience.com/an-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748↩︎ "]]
