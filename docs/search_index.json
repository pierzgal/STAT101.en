[["index.html", "STAT 101 IPS Lecture notes [WSMiP UŁ] Course description", " STAT 101 IPS Lecture notes [WSMiP UŁ] Michał Pierzgalski 2020-11-18 Course description Before you start reading, please look into the detailed syllabus: STAT101 "],["contact.html", "Contact", " Contact Email: michal.pierzgalski@uni.lodz.pl Personal website: https://pierzgal.github.io/michalpierzgalski/ Office hours (2020/2021) By appointment (via MS Teams); please, arrange the day and time via an official email address. IMPORTANT! Students, please contact me only via e-mail in the domain @uni.lodz.pl and @edu.uni.lodz.pl. In the email subject, please reference the course name, major, minor, etc. Students in contact with employees of the University of Lodz are required to use addresses in the domains of the University of Lodz and correspondence on the line student - teacher can only be conducted via these e-mail addresses. I reply to emails from students only on Fridays and Mondays or during my office hours. "],["introduction-lecture-1.html", "1 Introduction [Lecture 1]", " 1 Introduction [Lecture 1] Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. "],["motivation-two-reasons-why-you-should-learn-statistics-and-data-analysis-methods.html", "1.1 Motivation - two reasons why you should learn statistics and data analysis methods?", " 1.1 Motivation - two reasons why you should learn statistics and data analysis methods? In social research (political science, sociology, psychology or economics), statistical methods are used to find answers to research questions or to test research hypotheses. Statistics is a basic research tool for social scientists. Statistics and related disciplines such as Data Science or Machine Learning allow us to better understand modern technologies (such as Computational photography, Google’s Spam Filter, Self-driving cars) that not only make our lives easier, but also have an impact on politics and current society (e.g. the so-called microtargeting 1 - it “is a form of targeting that uses recent technological developments to gather large amounts of online data. The data from people’s digital footprint’s analyzed to create and convey messages that reflect an individual’s preferences and personality” (Wikipedia). Microtargeting (via social media like Facebook) can be used e.g. in election campaigns. Zob. np. Mikrotargetowanie i datakracja: https://wszystkoconajwazniejsze.pl/jan-sliwa-mikrotargetowanie-i-datakracja/↩︎ "],["review-of-some-mathematical-concepts-used-in-statistics-and-data-science.html", "1.2 Review of some mathematical concepts used in statistics and data science", " 1.2 Review of some mathematical concepts used in statistics and data science 1.2.1 Algebra review - some basic rules 1.2.1.1 Identity There are equalities of two different types: equations and identities. An equality \\(=\\) is not the same as identity \\(\\equiv\\). The \\(\\equiv\\) symbol originally meant “is identically equal to”. An identity is a type of equality which is true for all values, e.g.: \\((a+b)^2 \\equiv a^2+2ab+b^2.\\) 1.2.1.2 Differences - Delta notation The Greek letter \\(\\Delta\\) (capital delta) is the symbol used to indicate the difference in a measured quantity, usually at two different times. 1.2.1.3 Summation - Sigma notation In physics there are often contexts in which it’s necessary to add several quantities. A useful abbreviation for representing such a sum is the Greek letter \\(\\Sigma\\) (capital sigma). Suppose we wish to add a set of five numbers represented by \\(x1, x2, x3, x4, x5\\). In the abbreviated notation, we would write the sum as \\(x1 + x2 + x3 + x4 + x5 = \\sum_{i=1}^{5} x_i\\) where the subscript i on x represents any one of the numbers in the set. For example, if there are five masses in a system, \\(m1, m2, m3, m4, m5\\), the total mass of the system \\(M = m1 + m2 + m3 + m4 + m5\\) could be expressed as \\(M = \\sum_{i=1}^{5} m_i\\) 1.2.1.4 Absolute value The magnitude of a quantity x, written x, is simply the absolute value of that quantity. The sign of x is always positive, regardless of the sign of x. For example, if x = -5, then |x| = 5; if x = 8, then |x| = 8. 1.2.1.5 Basic algebraic operations When algebraic operations are performed, the laws of arithmetic apply. Symbols such as x, y, and z are usually used to represent unspecified quantities, called the unknowns (or variables). First, consider the equation \\(8x = 32\\) If we wish to solve for x, we can divide (or multiply) each side of the equation by the same factor without destroying the equality. In this case, if we divide both sides by 8, we have \\(\\frac{8x}{8} = \\frac{32}{8}\\) \\(x = 4\\) Also, consider the equation \\(x + 2 = 8\\) In this type of expression, we can add or subtract the same quantity from each side. If we subtract 2 from each side, we have \\(x + 2 - 2 = 8 - 2\\) \\(x = 6\\) Now, consider the equation \\(\\frac{x}{5} = 9\\) If we multiply each side by 5, \\((\\frac{x}{5}) \\times 5 = 9 \\times 5\\) \\(x = 45\\) In all cases, whatever operation is performed on the left side of the equality must also be performed on the right side. 1.2.1.6 Some useful arithmetic rules Należy przypomnieć następujące zasady mnożenia, dzielenia, dodawania i odejmowania ułamków, gdzie a, b, c i d to cztery liczby: Rule Example Multiplying \\((\\frac{a}{b})(\\frac{c}{d}) = \\frac{ac}{bd}\\) Dividing \\((\\frac{a/b}{c/d}) = \\frac{ad}{bc}\\) Adding \\((\\frac{a}{b}) \\pm (\\frac{c}{d}) = \\frac{ad \\pm bc}{bd}\\) 1.2.1.7 Powers The following rules apply: \\(x^nx^m = x^{n+m}\\) \\(\\frac{x^n}{x^m} = x^{n-m}\\) \\(x^{\\frac{1}{n}} = \\sqrt[n]{x}\\) \\((x^n)^m = x^{nm}\\) 1.2.1.8 Logarithms Suppose a quantity x is expressed as a power of some quantity a: \\(x = a^y\\) The number a is called the base number. The logarithm of x with respect to the base a is equal to the exponent to which the base must be raised to satisfy the expression \\(x = a^y\\): \\(y = \\log_a x\\) In practice, the two bases most often used are base 10, called the common logarithm base, and base \\(e = 2.718 282\\), called Euler’s constant or the natural logarithm base. For any base: \\(\\log{(ab)} = \\log a + \\log b\\) \\(\\log{a/b} = \\log a + \\log b\\) \\(\\log{a^n} = n \\log a\\) Also, \\(\\ln e\\) \\(\\ln e^a = a\\) \\(\\ln(1/a) = -\\ln a\\) 1.2.1.9 Factoring Some useful formulas for factoring an equation are the following: \\(ax + ay + az = a(x + y + z)\\) \\(a^2 + 2ab + b^2 = (a + b)^2\\) - so called “perfect square” \\(a^2 + b^2 = (a + b)(a - b)\\) 1.2.1.10 Linear equations A linear equation has the general form \\(y = mx + b\\) where m and b are constants. This equation is referred to as linear because the graph of y versus x is a straight line. The constant b, called the y-intercept, represents the value of y at which the straight line intersects the y axis. The constant m is equal to the slope of the straight line. If any two points on the straight line are specified by the coordinates (x1, y1) and (x2, y2), the slope of the straight line can be expressed as \\(Slope = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{\\Delta{y}}{\\Delta{x}}\\) Note that m and b can have either positive or negative values. If \\(m &gt; 0\\), the straight line has a positive slope. If \\(m &lt; 0\\), the straight line has a negative slope. Solving Simultaneous Linear Equations Consider the equation \\(3x + 5y = 15\\), which has two unknowns, x and y. Such an equation does not have a unique solution. For example, \\((x = 0, y = 3)\\), \\((x = 5, y = 0)\\), and (\\(x = 2, y = \\frac{9}{5}\\)) are all solutions to this equation. If a problem has two unknowns (variables), a unique solution is possible only if we have two equations. In general, if a problem has n unknowns, its solution requires n equations. To solve two simultaneous equations involving two unknowns, x and y, we solve one of the equations for x in terms of y and substitute this expression into the other equation. 1.2.2 Functions A function from a set A to another set B is an assignment of some element of B to each element in A. A function is a rule that assigns each input exactly one output. We call the output the image of the input. The set of all inputs for a function is called the domain. The set of all allowable outputs is called the codomain. In mathematics, a function is a binary relation between two sets that associates every element of the first set to exactly one element of the second set. Figure 1.1. Function2 Figure 1.2. Function3 A mathematical model is an abstract concept through which we use mathematical language and notation to describe a phenomenon in the world around us. Models describe our beliefs about how the world functions. In mathematical modelling, we translate those beliefs into the language of mathematics. A function can serve as a simple kind of mathematical model. Remember that a function is just a rule, f, that expresses the dependency of one variable quantity, y, on another variable quantity, x. We can think of the rule (given as a graph, a formula, or a table of values) as a representation of some natural cause and effect relationship – if x, then y – between the two variable quantities. For a mathematical model, we often seek an algebraic formula that captures observed behavior accurately and can be used to predict behavior not yet observed. 1.2.3 Sets Sets are denoted using curly braces \\(\\{\\}\\). e.g. \\(A = \\{0, 1\\}\\) 1.2.3.1 Set Definitions A set is a well-defined collection of objects. Each object in a set is called an element of the set. Two sets are equal if they have exactly the same elements in them. A set that contains no elements is called a null set or an empty set. If every element in Set A is also in Set B, then Set A is a subset of Set B. Membership: If \\(a\\) is a member of a set A, we write \\(a \\in A\\) Sets of numbers: set of natural numbers \\(N = \\{0, 1, 2, 3, ... \\}\\), set of positive natural numbers \\(N^+ = \\{1, 2, 3, ... \\}\\), a set of integers \\(Z = \\{..., -2, -1, 0, 1, 2, 3, ... \\}\\), a set of rational numbers \\(Q\\), i.e. all numbers that can be represented as \\(\\frac{p}{q}\\) for \\(q \\neq 0\\), set of irrational numbers \\(NQ\\), i.e. all numbers that have infinite and non-periodic decimal expansion, e.g. number \\(\\pi \\approx 3,141592...\\), or a number indicated by a letter \\(e \\approx 2,718 ...\\), set of real numbers \\(R = Q + NQ\\). \\(R\\) can be expressed as an interval \\((-\\infty, \\infty)\\). An interval is a set of real numbers with the property that any number that lies between two numbers in the set is also included in the set. The interval of numbers between a and b, including a and b, is often denoted \\([a, b]\\). The two numbers are called the endpoints of the interval. A singleton is a set with exactly one element. CAUTION: Be sure you understand the difference between the outcome -8 and the event {−8}, which is the set consisting of the single outcome −8. The cardinality (or size) of a collection or set \\(A\\), denoted \\(|A|\\), is the number of elements of the collection. This number may be finite or infinite. A finite set is a set that has a finite number of elements. In other words, it is either an empty set, a singleton, or a set whose elements can be listed in the form \\({a1, a2, . . . , an}\\) for some \\(n \\in N\\). A set that is not finite is called infinite. These sets have more than \\(n\\) elements for any integer \\(n\\). Whether finite or infinite, the elements of a countable set can always be counted one at a time and, although the counting may never finish, every element of the set is associated with a natural number. Countable sets form the foundation of a branch of mathematics called discrete mathematics. A set that is not countable is called uncountable set (or uncountably infinite set). It contains too many elements to be countable, e.g. an interval with positive length: \\([0, 1]\\). Table 1.1. The terminology of set theory and probability theory Set Theory Probability Theory Set Event Universal set Sample space Element Outcome Table 1.2. Probability event language - Event language \\(A\\) A occurs \\(A^c\\) A does not occur \\(A \\cup B\\) Either A or B occur \\(A \\cap B\\) Both A and B occur The difference between countable and uncountable sets is important for statistics and probability. Because of the mathematics required to determine probabilities, probabilistic methods are divided into two distinct types, discrete and continuous. A discrete approach is used when the number of experimental outcomes is finite (or infinite but countable). A continuous approach is used when the outcomes are continuous (and therefore infinite). It will be important to keep in mind which case is under consideration since otherwise, certain paradoxes may result. 1.2.4 Three basic concepts of calculus Limit of function; Derivative; Integral. 1.2.5 Limits In mathematics, a limit is the value that a function (or sequence) “approaches” as the input (or index) “approaches” some value. Limits are essential to calculus and mathematical analysis, and are used to define e.g. derivatives, and integrals. For \\(f(x)\\), we have \\[\\lim_{x \\rightarrow c}f(x)=L\\] For example, \\[\\lim_{x \\rightarrow 0}\\frac{1}{x}=\\infty\\] or, \\[\\lim_{x \\rightarrow \\infty}\\frac{1}{x}=0\\] In formulas, a limit of a function is usually written as \\[\\lim_{x \\to c}f(x)=L\\] and is read as “the limit of f of x as x approaches c equals L”. The fact that a function f approaches the limit L as x approaches c is usually denoted by a right arrow \\(\\to\\), as in: \\[f(x)\\to L{\\text{ as }}x\\to c\\] which reads \\(f(x)\\) tends to \\(L\\) as \\(x\\) tends to \\(c\\). Figure 1.3. Limits4 1.2.6 Derivative First, a function must be specified that relates one variable to another (e.g., a coordinate as a function of time). Suppose one of the variables is called y (the dependent variable), and the other x (the independent variable). We might have a function relationship such as \\(y(x) = ax^2 + bx + c\\) If a, b, and c are specified constants, y can be calculated for any value of x. We usually deal with continuous functions, that is, those for which y varies “smoothly” with x. The derivative of y with respect to x is defined as the limit as \\(\\Delta x\\) approaches zero of the slopes of chords drawn between two points on the y versus x curve. Mathematically, we write this definition as \\[\\frac{dy}{dx} = \\lim_{\\Delta x \\to\\ 0} \\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to\\ 0} \\frac{y(x + \\Delta x) - y(x)}{\\Delta x}\\] where \\(\\Delta y = y_2 - y_1\\) and \\(\\Delta x = x_2 - x_1\\). Note that \\(dy/dx\\) does not mean \\(dy\\) divided by \\(dx\\), but rather is simply a notation of the limiting process of the derivative (differentiation operator). A useful expression to remember when \\(y = ax^n\\), where a is a constant and n is any positive or negative number (integer or fraction), is \\(\\frac{dy}{dx} = nax^{n-1}\\) Figure 1.4. Derivative - visual representation5 1.2.7 Integral We think of integration as the inverse of differentiation. As an example, consider the expression 1.1 \\(f(x) = \\frac{dy}{dx} = 3ax^2 + b\\) which was the result of differentiating the function 1.2 \\(y(x) = ax^3 = bx + c\\) We can write equation 1.1 as \\(dy = f(x)dx = (3ax^2 + b)dx\\) and obtain \\(y(x)\\) by “summing” over all values of x. Mathematically, we write this inverse operation as \\[y(x) = \\int f(x) dx\\] \\[ y(x) = \\int (3az^2 + b)dx = ax^3 + bx + c\\] where c is a constant of the integration. This type of integral is called an indefinite integral because its value depends on the choice of c. 1.2.8 Integral as an area - intuitive explanation For a general continuous function f(x), the integral can be described as the area under the curve bounded by f(x) and the x axis, between two specified values of x, say, \\(x1\\) and \\(x2\\), as in the Figure 1.5. The area of the blue element in Figure 1.5 is approximately \\(f(x_i) \\Delta x_i\\). If we sum all these area elements between \\(x1\\) and \\(x2\\) and take the limit of this sum as \\(\\Delta x_i \\to 0\\), we obtain the true area under the curve bounded by f(x) and the x axis, between the limits \\(x1\\) and \\(x2\\): \\[Area = \\lim_{\\Delta x_i \\to\\ 0} \\sum_i f(x_i)\\Delta x_i = \\int_{x_1}^{x_2} f(x)dx\\] Integrals of this type are called definite integrals. Figure 1.5. Definite integral as an area under a curve6 Figure 1.6. Definite integral as an area under a curve7 Figure 1.7. Definite integral as an area under a curve8 1.2.8.1 Summary Figure 1.8. Summary of basic calulus concepts9 1.2.9 Probability Probability is the bedrock of statistics and data science. Later, we will review the basics of probability theory in detail. Retrieved from: Sets and Functions. https://mathigon.org/course/sets-and-functions/function-properties↩︎ Retrieved from: Loup Vaillant. http://loup-vaillant.fr/tutorials/from-imperative-to-functional↩︎ Retrieved from: Math24. https://www.math24.net/definition-limit-function/↩︎ Retrieved from: Wikimedia. https://commons.wikimedia.org/wiki/File:Derivative_-_geometric_meaning.svg↩︎ Retrieved from: Serway, R. A., &amp; Jewett, J. W. (2018). Physics for scientists and engineers with modern physics. Cengage learning.↩︎ Retrieved from: Active Calculus. https://activecalculus.org/single/sec-4-3-definite-integral.html↩︎ Retrieved from: Active Calculus. https://activecalculus.org/single/sec-4-3-definite-integral.html↩︎ Retrieved from: Derivatives on Unequally Spaced Grids. http://cococubed.asu.edu/code_pages/fdcoef.shtml↩︎ "],["statistics-an-introduction-lecture-2.html", "2 Statistics - an introduction [Lecture 2]", " 2 Statistics - an introduction [Lecture 2] Statistics is the science of learning from data. Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys or experiments. "],["essential-concepts.html", "2.1 Essential concepts", " 2.1 Essential concepts Data refers to a set of values, which are usually organized by variables (what is being measured) and observational units (members of the sample/population). An example of data is a data matrix in a spreadsheet program, such as Google Sheets. A collection of observations on one or more variables. Variable: A characteristic whose value may change from one observation to another. When we want to talk about the influence of a factor on a characteristic of interest, we identify the factor (a variable) as the independent variable (often called a predictor or explanatory variable) and the affected variable as the dependent variable (often called the response variable). Population: The entire collection of individuals or objects about which information is desired is called the population of interest. Sample: A sample is a subset of the population, selected for study. 2.1.1 A parameter and a statistic A parameter is a value, usually a numerical value, that describes a population. A parameter is usually derived from measurements of the individuals in the population. A statistic is a value, usually a numerical value, that describes a sample. A statistic is usually derived from measurements of the individuals in the sample. "],["constructs-and-operational-definitionslecture-1.html", "2.2 Constructs and operational definitions10", " 2.2 Constructs and operational definitions10 “Some variables, such as height, weight, and eye color are well-defined, concrete entities that can be observed and measured directly. On the other hand, many variables studied by behavioral scientists are internal characteristics that people use to help describe and explain behavior. For example, we say that a student does well in school because he or she is intelligent. Or we say that someone is anxious in social situations, or that someone seems to be hungry. Variables like intelligence, anxiety, and hunger are called constructs, and because they are intangible and cannot be directly observed, they are often called hypothetical constructs.” Constructs are internal attributes or characteristics that cannot be directly observed but are useful for describing and explaining behavior. An operational definition identifies a measurement procedure (a set of operations) for measuring an external behavior and uses the resulting measurements as a definition and a measurement of a hypothetical construct. Note that an operational definition has two components. First, it describes a set of operations for measuring a construct. Second, it defines the construct in terms of the resulting measurements. 2.2.1 Conceptualisation and operationalisation Conceptualization is the mental process by which fuzzy and imprecise constructs (concepts) and their constituent components are defined in concrete and precise terms. For example, the process of understanding what is included and what is excluded in the concept of e.g. “prejudice” is the conceptualization process. The conceptualization process is all the more important because of the imprecision, vagueness, and ambiguity of many social science constructs. While defining constructs such as prejudice or compassion, we must understand that sometimes, these constructs are not real or can exist independently, but are simply imaginary creations in our mind. Once a theoretical construct is defined, exactly how do we measure it? Operationalization refers to the process of developing indicators or items for measuring these constructs. For instance, if an unobservable theoretical construct such as socioeconomic status is defined as the level of family income, it can be operationalized using an indicator that asks respondents the question: what is your annual family income? Given the high level of subjectivity and imprecision inherent in social science constructs, we tend to measure most of those constructs (except a few demographic constructs such as age, gender, education, and income) using multiple indicators. This process allows us to examine the closeness amongst these indicators as an assessment of their accuracy (reliability). Retrieved from: Gravetter, F.J., Wallnau, L.B., Forzano, L.A.B. and Witnauer, J.E., 2020. *Essentials of statistics for the behavioral sciences*. Cengage Learning.↩︎ "],["data-collection.html", "2.3 Data collection", " 2.3 Data collection Generally, you can obtain data in three different ways: 1. From a published source; 2. From a designed experiment; 3. From an observational study (e.g., a survey). Moreover, you can obtain data using a simulation: A simulation is the use of a mathematical or physical model to reproduce the conditions of a situation or process. Collecting data often involves the use of computers. Simulations allow you to study situations that are impractical or even dangerous to create in real life, and often they save time and money. For instance, automobile manufacturers use simulations with dummies to study the effects of crashes on humans. Throughout this course, you will have the opportunity to use applets that simulate statistical processes on a computer. "],["sampling-techniqueslecture-2.html", "2.4 Sampling techniques11", " 2.4 Sampling techniques11 A census is a count or measure of an entire population. Taking a census provides complete information, but it is often costly and difficult to perform. A sampling is a count or measure of part of a population, and is more commonly used in statistical studies. To collect unbiased data, a researcher must ensure that the sample is representative of the population. Appropriate sampling techniques must be used to ensure that inferences about the population are valid. Remember that when a study is done with faulty data, the results are questionable. Even with the best methods of sampling, a sampling error may occur. A sampling error is the difference between the results of a sample and those of the population. When you learn about inferential statistics, you will learn techniques of controlling sampling errors. A random sample is one in which every member of the population has an equal chance of being selected. A simple random sample is a sample in which every possible sample of the same size has the same chance of being selected. One way to collect a simple random sample is to assign a different number to each member of the population and then use a random number table Larson, R. and Farber, B., 2019. Elementary statistics. Pearson Education Canada.↩︎ "],["data-distribution.html", "2.5 Data distribution", " 2.5 Data distribution The first step in analyzing data collected on a variable is to look at the observed values by using graphs and numerical summaries. The goal is to describe key features of the distribution of a variable. The most crucial step to exploratory data analysis is estimating the distribution of a variable. The distribution of a variable describes how the observations fall (are distributed) across the range of possible values. The distribution of a data set is a table, graph, or formula that provides the values of the observations and how often they occur. An important aspect of the distribution of a quantitative data set is its shape. Indeed, the shape of a distribution frequently plays a role in determining the appropriate method of statistical analysis. To identify the shape of a distribution, the best approach usually is to use a smooth curve that approximates the overall shape. A frequency distribution is a collection of observations produced by sorting observations into classes and showing their frequency of occurrence in each class: The (frequency) distribution of data - name the possible observations (numbers, categories) and tell how frequently each occurs. A frequency distribution is a summary table in which the data are arranged into numerically ordered classes or intervals. A relative frequency distribution is obtained by dividing the frequency in each class by the total number of values. From this a percentage distribution can be obtained by multiplying each relative frequency by 100%. Frequency distribution for categorical data: A table that displays the possible categories along with the associated frequencies and/or relative frequencies. Frequency: The frequency for a particular category is the number of times the category appears in the data set. The relative frequency for a particular category is the proportion of the observations that belong to that category. "],["basic-stages-of-social-researchlecture-3.html", "2.6 Basic stages of social research12", " 2.6 Basic stages of social research12 Systematically testing our ideas about the nature of social reality often demands carefully planned and executed research in which the following occur: The problem to be studied is reduced to a testable hypothesis (for example, “One-parent families generate more delinquency than two-parent families”). An appropriate set of instruments is developed (for example, a questionnaire or an interview schedule). The data are collected (that is, the researcher might go into the field and conduct a poll or a survey). The data are analyzed for their bearing on the initial hypotheses. Results of the analysis are interpreted and communicated to an audience (for example, by means of a lecture, journal article, or press release). Retrieved from: Fox, J.A., Levin, J. and Forde, D.R., 2017. Elementary Statistics in Social Research.↩︎ "],["statistical-study-essential-steps.html", "2.7 Statistical study - essential steps", " 2.7 Statistical study - essential steps Identify the variable(s) of interest (the focus) and the population of the study. Develop a detailed plan for collecting data. If you use a sample, make sure the sample is representative of the population. Collect the data. Describe the data, using descriptive statistics techniques. Interpret the data and make decisions about the population using inferential statistics. Identify any possible errors. "],["experimental-and-observational-studieslecture-4.html", "2.8 Experimental and observational studies13", " 2.8 Experimental and observational studies13 A statistical study can usually be categorized as an observational study or an experiment. In an observational study, a researcher does not influence the responses. In an experiment, a researcher deliberately applies a treatment before observing the responses. Here is a brief summary of these types of studies. In an observational study, a researcher observes and measures characteristics of interest of part of a population but does not change existing conditions. For instance, an observational study was performed in which researchers observed and recorded the mouthing behavior on nonfood objects of children up to three years old. (Source: Pediatrics Magazine) In performing an experiment, a treatment is applied to part of a population, called a treatment group, and responses are observed. Another part of the population may be used as a control group, in which no treatment is applied. (The subjects in the treatment and control groups are called experimental units.) In many cases, subjects in the control group are given a placebo, which is a harmless, fake treatment, that is made to look like the real treatment. The responses of the treatment group and control group can then be compared and studied. In most cases, it is a good idea to use the same number of subjects for each group. For instance, an experiment was performed in which diabetics took cinnamon extract daily while a control group took none. After 40 days, the diabetics who took the cinnamon reduced their risk of heart disease while the control group experienced no change. (Source: Diabetes Care) Retrieved from: Larson, R., Farber, E. and Farber, E., 2009. Elementary statistics: Picturing the world. Pearson Prentice Hall.↩︎ "],["statistical-research-design-an-example-of-a-simple-experimentlecture-5.html", "2.9 Statistical research design - an example of a simple experiment14", " 2.9 Statistical research design - an example of a simple experiment14 Experimental research design - overview Gravetter, F.J., Wallnau, L.B., Forzano, L.A.B. and Witnauer, J.E., 2020. Essentials of statistics for the behavioral sciences. Cengage Learning.↩︎ "],["descriptive-and-inferential-statistics.html", "2.10 Descriptive and inferential statistics", " 2.10 Descriptive and inferential statistics Descriptive statistics: The branch of statistics that includes methods for organizing and summarizing data. Statistical tools and ideas help us examine data to describe their main features. This examination is called exploratory data analysis. Here are two basic strategies that help us organize our exploration of a set of data: Begin by examining each variable by itself. Then move on to study the relationships among the variables. Begin with a graph or graphs. Then add numerical summaries of specific aspects of the data. Inferential statistics: The branch of statistics that involves generalizing from a sample to the population from which the sample was selected and assessing the reliability of such generalizations. "],["random-samplinglecture-6.html", "2.11 Random sampling15", " 2.11 Random sampling15 A random sample is one that is selected without bias. In selecting a sample, the importance of randomness cannot be overemphasized. In statistical thinking we expect a random sample to share approximately the same properties as the population. Also, the larger the sample size, the more closely the sample properties approximate those of the population. For example, to estimate the average height of adult males, it would be highly biased to select a sample from among NBA players. In general, non-random samples are generated when there is bias in the selection process. Such samples are useless for statistical purposes, because the sample is not representative of the population. A simple random sample is one in which every individual in the population has the same probability of being selected. To satisfy this requirement, the sampling method that is used must be free of bias with respect to the property being measured. The “lottery method” of selection appears to produce a simple random sample (see the margin). Computer-generated random numbers (also called pseudo-random numbers) can be used in selecting random samples: We first assign a number to each individual in the population and then use a computer random number generator to select from the assigned numbers. The following are common types of sampling bias. 2.11.1 Sampling errors and systematic bias In statistics, sampling errors are incurred when the statistical characteristics of a population are estimated from a subset, or sample, of that population. Since the sample does not include all members of the population, statistics on the sample, such as means and quartiles, generally differ from the characteristics of the entire population, which are known as parameters. For example, if one measures the height of a thousand individuals from a country of one million, the average height of the thousand is typically not the same as the average height of all one million people in the country. Since sampling is typically done to determine the characteristics of a whole population, the difference between the sample and population values is considered an error. Systematic error is predictable and typically constant or proportional to the true value. If the cause of the systematic error can be identified, then it usually can be eliminated. Systematic errors are caused by imperfect calibration of measurement instruments or imperfect methods of observation, or interference of the environment with the measurement process, and always affect the results of an experiment in a predictable direction. 2.11.2 Various types of bias in statistical analysis Undercoverage bias (or exclusion bias), in which part of the population is excluded from the sampling process. Response bias, in which the wording of a questionnaire is not neutral but rather suggests or provokes a particular response. Nonresponse bias, in which individuals with a common characteristic are unwilling (or neglect) to respond to a questionnaire. (Notice that this is not the opposite of response bias.) Self-selection bias (or voluntary response bias), in which individuals select themselves (or volunteer) for the sample. Many of these biases are a result of convenience sampling, in which individuals are sampled only because they are nearby or easily accessible. Selection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. The phrase “selection bias” most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may be false. In practice, it is difficult to ensure the selection of a random sample. Several methods have been developed for sampling in particular situations. Some of the most common are as follows. Systematic Sampling: In systematic sampling, a sample is chosen systematically from a list. For example, we may pick every 100th name in a telephone book. Stratified Sampling: In stratified sampling, the population is first divided into nonoverlapping groups (or strata), and then the sample is chosen proportionally from each group. For example, to pick a sample of registered voters, we may want to stratify the population into groups—white, African American, Hispanic, and other—and then randomly pick registered voters, choosing from each group a number proportional to the size of the group in the population. Cluster Sampling: In cluster sampling, the population is divided into groups (or clusters) and then a random sample of clusters is selected. For example, to survey apartment dwellers in Los Angeles, we would first randomly select a collection of apartment buildings (the clusters) and then interview every resident in the selected buildings. This type of sampling reduces the enormous time and cost for the pollster in traveling from apartment to apartment. Retrieved from: Stewart, J., Redlin, L. and Watson, S., 2013. Precalculus: Mathematics for calculus. Cengage Learning.↩︎ "],["experimental-designlecture-7.html", "2.12 Experimental design16", " 2.12 Experimental design16 In observational studies, the researcher has no control over the factors affecting the property being studied—the researcher is merely an observer. Extraneous or unintended variables that systematically affect the property being studied are called confounding variables (or confounding factors or lurking variables). Such variables are said to confound (or mix up) the results of the study. The following examples show how this can happen. To eliminate or vastly reduce the effects of confounding variables, researchers often conduct experiments so that such variables can be controlled. In an experimental study, two groups are selected: a treatment group (in which individuals are given a treatment) and a control group (in which individuals are not given the treatment). The individuals in the experiment are called subjects (or experimental units). The goal is to measure the response of the subjects to the treatment—that is, whether or not the treatment has an effect. The next step is to make sure that the two groups are as similar as possible except for the treatment. If the two groups are alike except for the treatment, then any statistical difference in response between the groups can be confidently attributed to the treatment. Here are some typical experimental designs. 1. Completely Randomized Design: The effects of unknown variables that may confound the experiment can be reduced or eliminated by randomization, that is, by assigning individuals randomly to the treatment or control groups. Randomization ensures that the effects of any confounding variables are equally likely to occur in either group. So any difference between the two groups in the response to the treatment can be attributed to the treatment. 2. Randomized Block Design: Variables that are known (prior to the experiment) to affect the response can be controlled by blocking. The participants are arranged into blocks (or groups) consisting of subjects with similar characteristics, and then treatment and control groups are randomly selected within each block. For example, if sex is a known source of variability in response, then a male block and a female block are formed. Treatment and control subjects are then randomly selected from the male block and from the female block. This design specifically “controls” for a known confounding factor by ensuring equal participation of individuals from each block in the treatment and control groups. 3. Matched Pair Design: In this design, the subjects are matched in pairs based on variables that may affect the response to the treatment. For example, the subjects of the study may be matched in pairs based on age and sex (a young male with a young male, a senior female with a senior female, and so on). Then an individual from each pair is randomly assigned to the treatment group, and the other is assigned to the control group. In this example, the treatment and control groups have equal participation with respect to two possible confounding variables (age and sex). A common confounding factor is the placebo effect, in which patients who think they are receiving a medication report an improvement (perceived or actual), even though the “treatment” they received was a placebo—a simulated or false treatment (sometimes called a “sugar pill”). To control the placebo effect, a researcher may use single-blinding, a method in which subjects don’t know whether they are in the treatment or control group, or double-blinding, in which the researchers are also not privy to this information during the course of the experiment. Replication, the repetition of the experiment, can also reinforce the reliability of the results. Retrieved from: Stewart, J., Redlin, L. and Watson, S., 2013. Precalculus: Mathematics for calculus. Cengage Learning.↩︎ "],["relevant-modern-application-of-statistical-theory-machine-learning-the-conceptual-introduction.html", "2.13 Relevant modern application of statistical theory: Machine learning - the conceptual introduction", " 2.13 Relevant modern application of statistical theory: Machine learning - the conceptual introduction Definitions Machine learning (ML) is a sub-field of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning algorithms build a mathematical model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to do so. Basic concepts used in Machine Learning: Algorithm: A Machine Learning algorithm is a set of rules and statistical techniques used to learn patterns from data and draw significant information from it. It is the logic behind a Machine Learning model. An example of a Machine Learning algorithm is the Linear Regression algorithm. Model: A model is the main component of Machine Learning. A model is trained by using a Machine Learning Algorithm. An algorithm maps all the decisions that a model is supposed to take based on the given input, in order to get the correct output. Predictor Variable: It is a feature(s) of the data that can be used to predict the output. Response Variable: It is the feature or the output variable that needs to be predicted by using the predictor variable(s). Training Data: The Machine Learning model is built using the training data. The training data helps the model to identify key trends and patterns essential to predict the output. Testing Data: After the model is trained, it must be tested to evaluate how accurately it can predict an outcome. This is done by the testing data set. 2.13.1 Machine learning approaches DiagrammeR::grViz(&quot;digraph { graph [layout = dot, rankdir = TB] node [shape = rectangle] rec1 [label = &#39;Step 1. Machine learning&#39;] rec2 [label = &#39;Step 2. Supervised learning&#39;] rec3 [label = &#39;Step 3. Unsupervised learning&#39;] rec4 [label = &#39;Step 4. Reinforced learning&#39;] # edge definitions with the node IDs rec1 -&gt; {rec2, rec3, rec4} }&quot;, height = 200) [From Wikipedia.org:] Supervised learning: The computer is presented with example inputs and their desired outputs, given by a “teacher”, and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that’s analogous to rewards, which it tries to maximize. “For the process of learning (model fitting) we need to have available some observations or data (also known as samples or examples) in order to explore potential underlying patterns, hidden in our data. These learned patterns are nothing more that some functions or decision boundaries.” 2.13.2 Make predictions or decisions using ML In science and technology, predictions or decisions are made using formal models. A very simple mathematical deterministic model can be: \\[F = \\frac{9}{5}(C) + 32\\] which you could use to convert degrees Celsius to degrees Fahrenheit. For example: \\[F = \\frac{9}{5}(39) + 32 = 70.2 + 32 = 102.2\\] You might be familiar with the formula for a line using the slope and y-intercept (recall the equation of linear function - \\(y = ax + b\\)): \\(y\\) = predicted value (dependent variable); \\(a\\) = slope, i.e. \\(\\frac{\\text{change in }y}{\\text{change in }x}\\); \\(b\\) = intercept with \\(x-\\)axis. However, in statistics, we rarely deal with deterministic models which will produce perfect prediction. We have stochastic models which will have unexplained (“random”) error. If you have an equation that tries to predict what the adult height of a newborn baby boy based on the height of his father. You will certainly not get a perfect prediction for many reasons. A mathematical version of a simple linear regression model, which is stochastic, would be: \\[y = \\beta_0 + x_1\\beta_1 + ... + x_k\\beta_k + \\epsilon\\] This denotes that we believe that there is a linear relationship between predictor variables \\(x_k\\) and response variable \\(y\\) with some unexplained error \\(\\epsilon\\). 2.13.3 Machine learning approaches - visual guide The following figures give general idea on how machine learning works17: Supervised learning Unsupervised learning Reinforcement learning 2.13.3.1 Supervised learning - example18 We seek to teach a computer to predict housing prices. We begin with giving our computer the prices of other houses in the area, as well as information about each house (e.g. the size, number of bedrooms, number of floors). This data are called the training set. Also, the data provided for any one of the given houses is called a training example - this is denoted by \\(x^{(i)}\\), meaning data pertaining to house (i), while \\(x^{(2)}\\) = training example (2). Each distinct bit of information contained within a training example is called a feature. In data representing housing attributes, the size of the house is a feature, as would the number of stories and the number of bedrooms. Every training example contains the same features in the same order. Features are denoted with \\(j\\). That is: \\(x^{(i)}_j\\) is a feature \\(j\\) for an example \\(i\\). Linear regression is one of the most basic machine learning models. Symbolically, this is a common representation of the linear regression model: \\[h_{\\theta}(x) = \\theta_0 + \\theta_1 x\\] \\(h_{\\theta}(x)\\) represents the hypothesis function (hypothesis). It says that the outputted value varies based on the input \\(x\\) That is to say, based on the value \\(x\\) between parentheses, the prediction is made, meaning that \\(h_{\\theta}(x^{(i)})\\) symbolizes the prediction made by the hypothesis on example \\(x^{(i)}\\). The \\(\\theta\\) symbols are known as parameters - can be positive or negative numbers. Also, theta zero is called a bias unit. “Let’s go back to our housing prices example. Let’s pretend we’re using training example 1. Our example will use only one feature for this explanation: the size of the house in square feet. This will be represented by \\(x^{(1)}_1\\), meaning feature 1 of training example 1, and will be equal to 2,500 (square feet).” \"Let’s say our hypothesis believes that each square foot adds $50 of value to the home and that housing prices start at $200,000 in the area. \\(\\theta_0\\) would then have a value of 200,000 and \\(\\theta_1\\) would then be 50. Let’s check out why that is. We know that a prediction is formed by summing the bias unit with the product of features and their respective parameters. When we multiply our \\(x^{(1)}_1\\) (2,500 square feet) by our \\(\\theta_1\\) (the associated parameter of $50/square feet), we get a value of $125,000. Through this multiplication, for every square foot (\\(x^{(1)}_1\\)), $50 dollars of value (\\(\\theta_1\\)) is added. We add that product to our bias unit of $200,000 (\\(\\theta_0\\) representing the average local starting housing price) to get a final output value of $325,000!\" Retrieved from: Loukas, S. What is Machine Learning: Supervised, Unsupervised, Semi-Supervised and Reinforcement learning methods. https://towardsdatascience.com/what-is-machine-learning-a-short-note-on-supervised-unsupervised-semi-supervised-and-aed1573ae9bb↩︎ Based on: Nolan, F. A Comprehensive Introductory Guide to Supervised Learning for the Non-Mathematician. https://towardsdatascience.com/an-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748↩︎ "],["exploratory-data-analysis-lecture-3.html", "3 Exploratory data analysis [Lecture 3] ", " 3 Exploratory data analysis [Lecture 3] "],["types-of-data-and-the-scales-of-measurement.html", "3.1 Types of data and the scales of measurement", " 3.1 Types of data and the scales of measurement 3.1.1 What is measurement? Measurement is a method of assigning numbers to magnitudes. Bertrand Russell stated that measurement is any method by which a unique and reciprocal correspondence is established between all or some of the magnitudes of a kind and all or some of the numbers, integral, rational or real. (1903: 176) “Broadly speaking, measurement theory sets out to (i) identify the assumptions underlying the use of various mathematical structures for describing aspects of the empirical world, and (ii) draw lessons about the adequacy and limits of using these mathematical structures for describing aspects of the empirical world. Following Otto Hölder (1901), measurement theorists often tackle these goals through formal proofs, with the assumptions in (i) serving as axioms and the lessons in (ii) following as theorems. A key insight of measurement theory is that the empirically significant aspects of a given mathematical structure are those that mirror relevant relations among the objects being measured. For example, the relation”bigger than\" among numbers is empirically significant for measuring length insofar as it mirrors the relation “longer than” among objects. This mirroring, or mapping, of relations between objects and mathematical entities constitutes a measurement scale. As will be clarified below, measurement scales are usually thought of as isomorphisms or homomorphisms between objects and mathematical entities\". (…) “Mathematical theories of measurement (often referred to collectively as”measurement theory“) concern the conditions under which relations among numbers (and other mathematical entities) can be used to express relations among objects. In order to appreciate the need for mathematical theories of measurement, consider the fact that relations exhibited by numbers—such as equality, sum, difference and ratio—do not always correspond to relations among the objects measured by those numbers. For example, 60 is twice 30, but one would be mistaken in thinking that an object measured at 60 degrees Celsius is twice as hot as an object at 30 degrees Celsius. This is because the zero point of the Celsius scale is arbitrary and does not correspond to an absence of temperature. Similarly, numerical intervals do not always carry empirical information. When subjects are asked to rank on a scale from 1 to 7 how strongly they agree with a given statement, there is no prima facie reason to think that the intervals between 5 and 6 and between 6 and 7 correspond to equal increments of strength of opinion. To provide a third example, equality among numbers is transitive [if (a=b &amp; b=c) then a=c] but empirical comparisons among physical magnitudes reveal only approximate equality, which is not a transitive relation. These examples suggest that not all of the mathematical relations among numbers used in measurement are empirically significant, and that different kinds of measurement scale convey different kinds of empirically significant information”19. 3.1.2 Levels of measurement (scales of measurement) Adapted from: D. Osherson and D. M. Lane, http://onlinestatbook.com/2/introduction/levels_of_measurement.html Types of Scales Before we can conduct a statistical analysis, we need to measure our dependent variable. Exactly how the measurement is carried out depends on the type of variable involved in the analysis. Different types are measured differently. To measure the time taken to respond to a stimulus, you might use a stop watch. Stop watches are of no use, of course, when it comes to measuring someone’s attitude towards a political candidate. A rating scale is more appropriate in this case (with labels like very favorable, somewhat favorable, etc.). For a dependent variable such as favorite color, you can simply note the color-word (like red) that the subject offers. Although procedures for measurement differ in many ways, they can be classified using a few fundamental categories. In a given category, all of the procedures share some properties that are important for you to know about. The categories are called scale types, or just scales, and are described in this section. Nominal scales When measuring using a nominal scale, one simply names or categorizes responses. Gender, handedness, favorite color, and religion are examples of variables measured on a nominal scale. The essential point about nominal scales is that they do not imply any ordering among the responses. For example, when classifying people according to their favorite color, there is no sense in which green is placed “ahead of” blue. Responses are merely categorized. Nominal scales embody the lowest level of measurement. Ordinal scales A researcher wishing to measure consumers’ satisfaction with their microwave ovens might ask them to specify their feelings as either very dissatisfied, somewhat dissatisfied, somewhat satisfied, or very satisfied. The items in this scale are ordered, ranging from least to most satisfied. This is what distinguishes ordinal from nominal scales. Unlike nominal scales, ordinal scales allow comparisons of the degree to which two subjects possess the dependent variable. For example, our satisfaction ordering makes it meaningful to assert that one person is more satisfied than another with their microwave ovens. Such an assertion reflects the first person’s use of a verbal label that comes later in the list than the label chosen by the second person. On the other hand, ordinal scales fail to capture important information that will be present in the other scales we examine. In particular, the difference between two levels of an ordinal scale cannot be assumed to be the same as the difference between two other levels. In our satisfaction scale, for example, the difference between the responses very dissatisfied and somewhat dissatisfied is probably not equivalent to the difference between somewhat dissatisfied and somewhat satisfied. Nothing in our measurement procedure allows us to determine whether the two differences reflect the same difference in psychological satisfaction. Statisticians express this point by saying that the differences between adjacent scale values do not necessarily represent equal intervals on the underlying scale giving rise to the measurements. (In our case, the underlying scale is the true feeling of satisfaction, which we are trying to measure.) What if the researcher had measured satisfaction by asking consumers to indicate their level of satisfaction by choosing a number from one to four? Would the difference between the responses of one and two necessarily reflect the same difference in satisfaction as the difference between the responses two and three? The answer is No. Changing the response format to numbers does not change the meaning of the scale. We still are in no position to assert that the mental step from 1 to 2 (for example) is the same as the mental step from 3 to 4. Interval scales Interval scales are numerical scales in which intervals have the same interpretation throughout. As an example, consider the Fahrenheit scale of temperature. The difference between 30 degrees and 40 degrees represents the same temperature difference as the difference between 80 degrees and 90 degrees. This is because each 10-degree interval has the same physical meaning (in terms of the kinetic energy of molecules). Interval scales are not perfect, however. In particular, they do not have a true zero point even if one of the scaled values happens to carry the name “zero”. The Fahrenheit scale illustrates the issue. Zero degrees Fahrenheit does not represent the complete absence of temperature (the absence of any molecular kinetic energy). In reality, the label “zero” is applied to its temperature for quite accidental reasons connected to the history of temperature measurement. Since an interval scale has no true zero point, it does not make sense to compute ratios of temperatures. For example, there is no sense in which the ratio of 40 to 20 degrees Fahrenheit is the same as the ratio of 100 to 50 degrees; no interesting physical property is preserved across the two ratios. After all, if the “zero” label were applied at the temperature that Fahrenheit happens to label as 10 degrees, the two ratios would instead be 30 to 10 and 90 to 40, no longer the same! For this reason, it does not make sense to say that 80 degrees is “twice as hot” as 40 degrees. Such a claim would depend on an arbitrary decision about where to “start” the temperature scale, namely, what temperature to call zero (whereas the claim is intended to make a more fundamental assertion about the underlying physical reality). Ratio scales The ratio scale of measurement is the most informative scale. It is an interval scale with the additional property that its zero position indicates the absence of the quantity being measured. You can think of a ratio scale as the three earlier scales rolled up in one. Like a nominal scale, it provides a name or category for each object (the numbers serve as labels). Like an ordinal scale, the objects are ordered (in terms of the ordering of the numbers). Like an interval scale, the same difference at two places on the scale has the same meaning. And in addition, the same ratio at two places on the scale also carries the same meaning. The Fahrenheit scale for temperature has an arbitrary zero point and is therefore not a ratio scale. However, zero on the Kelvin scale is absolute zero. This makes the Kelvin scale a ratio scale. For example, if one temperature is twice as high as another as measured on the Kelvin scale, then it has twice the kinetic energy of the other temperature. Another example of a ratio scale is the amount of money you have in your pocket right now (25 cents, 55 cents, etc.). Money is measured on a ratio scale because, in addition to having the properties of an interval scale, it has a true zero point: if you have zero money, this implies the absence of money. Since money has a true zero point, it makes sense to say that someone with 50 cents has twice as much money as someone with 25 cents (or that Bill Gates has a million times more money than you do). 3.1.3 Discrete or continuous variables All qualitative data are discrete (nominal and ordinal scale). When it comes to the quantitative variables, we may distinguish: discrete variables, and continuous variables. Discrete data set is composed of isolated points (finitely or infinitely many) on the number line, while continuous sets are numerical intervals, e.g. (1, 10]. For the set \\(A = \\{0, 0.1, 2, (3, 10]\\}\\), number \\(0, 0.1, 2\\) are isolated points. The set A also contains an interval (3, 10], there are no isolated points between 3 and 10. If a variable takes values from a discrete set, e.g. -1, 0, 1, 2, 3 …, it is discrete variable, if a variable takes values from a continuous data set, its is continuous variable. Continuous variables can take any value from a numerical interval. A numerical interval (e.g. [1, 2.5]) always contains infinitely many elements. Moreover, the elements of any numerical interval are uncountable. Discrete sets, while sometimes infinite, are always countable. Besides, sometimes, even if, strictly speaking, a variable is discrete, it is easier to treat this variable as a continuous one - e.g. income per capita in Euro. We usually treat income as continuous variable, but it is, strictly speaking, discrete variable. Income is measured in a currency, e.g. Euro, and it cannot take any value. It takes values from the set that is composed of isolated points. We can earn 4000.52 Euro, but we cannot earn 4000.5256 Euro - a precision of measurement is limited to two decimal places. Variables that are measured in very small but discrete units (e.g. income, exam scores measured in points) are usually treated as continuous in practice; these variables usualy behave as continuous variables. Variables that are truly continuous (time, weight, height, etc.) may take any value, however, in practice, a measurement precision is limited, thus we cannot observe any value of a continuous variable. Retrieved from: https://plato.stanford.edu/entries/measurement-science/↩︎ "],["levels-of-measurement-a-summary.html", "3.2 Levels of measurement - a summary", " 3.2 Levels of measurement - a summary Type Qualitative Qualitative Quantitative Quantitative Subtype Nominal Ordinal Interval Ratio Basic features Distinct categories (e.g. gender) Ordered categories (e.g. Likert scales, IQ) Meaningful distances (e.g. dates, temperature measured using Celsius or Fahrenheit scales) Absolute zero (e.g. weight, height, age, voting turnout, sample size) Possible math operations \\(=, \\neq\\) \\(&gt;, &lt;\\) \\(+, -\\) \\(\\times, \\div\\) 3.2.1 Types of data and levels of measurement - intuitions Levels of measurement Continuous and Discrete 3.2.2 Frequency distribution A sequence of observation, made on a set of objects included in the sample drawn from population is known as statistical data. Main methods of data collection include census, sample data (including survey, experimental and quasi-experimental research), and also administrative by-product. A census refers to data collection about everyone or everything in a group or statistical population and has advantages such as accuracy and detail, and disadvantages such as cost and time. A sampling is a data collection method that includes only part of the total population and has advantages such as cost and time, and disadvantages such as accuracy and detail. Administrative by-product data are collected as a by-product of an organization’s day-to-day operations and has advantages such as accuracy, time and simplicity, and disadvantages such as no flexibility and lack of control. Collected, but unordered, data are stored in columns of databases (e.g. in MS Excel files, in SQL databases - eg. MySQL). In databases, each column represent a single variable, each row represents a single observation. Data may be organized into frequency distributions before we start to examine them. We can organize an unordered set of collected data into a frequency distribution. A frequency distribution is a tabular representation of a survey data set used to organize and summarize the data. Specifically, it is a list of either qualitative or quantitative values that a variable takes in a data set and the associated number of times each value occurs (frequencies). The frequency distribution is the basic building block of statistical analytical methods and the first step in analyzing survey data. It helps researchers (a) organize and summarize the survey data in a tabular format, (b) interpret the data, and (c) detect outliers (extreme and rare values) in the survey data set. "],["reliability-and-validity.html", "3.3 Reliability and Validity", " 3.3 Reliability and Validity Reliability and Validity Reliability refers to the consistency of a measure. Psychologists consider three types of consistency: over time (test-retest reliability), across items (internal consistency), and across different researchers (inter-rater reliability). Validity is the extent to which the scores from a measure represent the variable they are intended to. When a measure has good test-retest reliability and internal consistency, researchers should be more confident that the scores represent what they are supposed to. There has to be more to it, however, because a measure can be extremely reliable but have no validity whatsoever. “The usefulness of a measurement tool is reliant on the extent to which it can be considered reliable and accurate as an indicator of behavior. Reliability is an indication of the consistency of the measurement. The degree to which an instrument reflects what it is proposed to measure is reflected in validity. Internal consistency is a form of reliability. This property is most relevant to performance measures that consist of multiple items that are to be summarized clinically into a composite score. Clinical inferences made from a composite score of multiple items are strengthened by evidence that all items—in the case of the FGA, dynamic balance—are measuring the same construct.28 As an index of a test’s ability to differentiate among patients, a high degree of internal consistency also supports the use of the instrument as a screening tool”.20 Three essential types of validity Face validity is the extent to which a measurement method appears “on its face” to measure the construct of interest. Content validity is the extent to which a measure “covers” the construct of interest. Criterion validity is the extent to which people’s scores on a measure are correlated with other variables (known as criteria) that one would expect them to be correlated with. Retrieved from: https://opentextbc.ca/researchmethods/chapter/reliability-and-validity-of-measurement/↩︎ "],["tabular-presentation-of-data-distributions.html", "3.4 Tabular presentation of data distributions", " 3.4 Tabular presentation of data distributions 3.4.1 Unordered data \\(X = \\{4,4,4,4,8,8,8,2\\}\\) \\(i\\) \\(x_i\\) 1 4 2 4 3 4 4 4 5 8 6 8 7 8 8 2 - - \\(n = 8\\) "],["frequency-distribution-1.html", "3.5 Frequency distribution", " 3.5 Frequency distribution The frequency distribution of the variable tells us what values it takes and how often it takes these values. \\(X = \\{4,4,4,4,8,8,8,2\\}\\) Id \\(x_i\\) Frequency \\(f_i\\) Relative frequency (fractions) \\(p_i = f_i/n\\) 1 2 1 1/8 2 4 4 1/2 3 8 3 3/8 \\(\\Sigma\\) - \\(n\\) = 8 1 Remark: The relative frequencies may be used to estimate probabilities of data points (the empirical or statistical probabilities), denoted with the letter p. "],["frequency-distribution-with-intervals-for-grouped-data.html", "3.6 Frequency distribution with intervals (for grouped data)", " 3.6 Frequency distribution with intervals (for grouped data) Id Class interval (bin) \\(h_k\\) for \\(x_i\\) Frequency \\(f_i\\) Relative frequency \\(p_i = f_i/n\\) Density \\(d_i = p_i/\\Delta x_i\\) 1 [1, 6) 12 12/120 = 0.1 0.1/(6 - 1) = 0.02 2 [6, 11) 6 0.05 0.01 … … … … … 20 [96, 101) 3 0.025 0.005 \\(\\Sigma\\) - \\(n\\) = 120 1 - The concept of density (for an interval) is very similar to mass density in physics: its unit is probability per unit length. This is very Density scale is very useful in statistics; it gives the relative frequency per unit for the data in this class, where the unit is the unit of measurement of the data. This allows for a meaningful comparison of different classes where the class widths may not be equal. Histograms (the essential type of statistical graphs discussed below) have been a popular visualization option since at least the 18th century, in part because they are easily generated by hand. More recently, as extensive computing power has become available in everyday devices such as laptops and cell phones, we see them increasingly being replaced by the so-called density plots (smooth curves that model the shape of a histogram). Retrieved from: https://ajsmit.github.io/Basic_stats/graphical-data-displays.html "],["basic-descriptive-statistics.html", "3.7 Basic descriptive statistics", " 3.7 Basic descriptive statistics Any time a data set is summarized by its statistical information, there is a loss of information. That is, given the summary statistics, there is no way to recover the original data. Basic summary statistics may be grouped as: measures of central tendency (giving in some sense the central value of a data set) and measures of dispersion (giving a measure of how spread out that data set is). 3.7.1 Basic measures of central tendency mean (also known as expected value or average value), median - a median is a number separating the higher half of a data sample, a population, or a probability distribution, from the lower half. mode - the mode is the value that appears most often in a set of data. 3.7.2 Which measures of central tendency are appropriate for numerical and categorical data If the data are numerical or quantitative in nature, then the mean or median should be used to describe the center of the distribution. If the data do not contain any unusually large or unusually small values, then the mean should be used to describe the center of the distribution. If the data do contain unusually large or unusually small values, then the median should be used to describe the center of the distribution. If the data are purely categorical or qualitative, the mode should be used; arithmetic is not possible. However, if the data are ordinal, the median can be used as well. 3.7.3 Basic measures of spread/dispersion variance, standard deviation, interquartile range. "],["mean-and-standard-deviation.html", "3.8 Mean and standard deviation", " 3.8 Mean and standard deviation 3.8.1 Mean 3.8.2 Variance and standard deviation The figure above shows the three data sets (1, 2, 3); each of them has the same mean and median. However, the average dispersion of observations (data) relative to the mean, i.e. the standard deviation, is greatest for the lower set (3) and smallest for the uppermost set (1). Also, Figure 2.10 shows the income distributions for music teachers in the US and Denmark - Denmark’s standard deviation of income is much smaller, with the same mean value. "],["useful-formulas-descriptive-statistics.html", "3.9 Useful formulas (descriptive statistics)", " 3.9 Useful formulas (descriptive statistics) Id Unordered data distribution Type of available data - relative or absolute frequency Frequency distribution (ordered data distribution) 1 \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\] \\(n\\) = number of observations; \\[S^2(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\] \\[S(X) = \\sqrt{S^2(X)}\\] Absolute frequencies - \\(f_i\\) \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{k}x_if_i\\] \\(n\\) = number of observations (sample or population size); \\(k\\) = number of distinct values (# of rows in a frequency distribution). \\[S^2(X) = \\frac{1}{n}\\sum_{i=1}^{k}(x_i - \\bar{x})^2f_i\\] \\[S(X) = \\sqrt{S^2(X)}\\] or you can use formulas for the relative frequency of observations \\[\\bar{x} = \\sum_{i=1}^{k}x_ip_i\\] \\[S^2(X) = \\sum_{i=1}^{k}(x_i - \\bar{x})^2p_i\\] \\[S(X) = \\sqrt{S^2(X)}\\] 2 - Relative frequencies (fraction, proportion) - \\(p_i = f_i/n\\) \\[\\bar{x} = \\sum_{i=1}^{k}x_ip_i\\] \\(n\\) = number of observations; \\(k\\) = number of distinct values (groups). \\[S^2(X) = \\sum_{i=1}^{k}(x_i - \\bar{x})^2p_i\\] \\[S(X) = \\sqrt{S^2(X)}\\] "],["parameter-and-statistic-summary-of-symbols.html", "3.10 Parameter and statistic - summary of symbols", " 3.10 Parameter and statistic - summary of symbols Population (P) Sample (S) expected value (mean): \\(E(X), \\mu\\) sample mean: \\(\\bar{x}\\) proportion: sample proportion: \\(\\hat{p}\\) standard deviation: \\(\\sigma\\) sample standard deviation: \\(S, S(X), s\\) variance: \\(\\sigma^2\\), \\(Var(X)\\) sample variance: \\(S^2, S^2(X), s^2\\) median: \\(Md_X\\) sample median \\(\\hat{Md}_X\\) mode: \\(Mo_X\\) sample mode \\(\\hat{Mo}_X\\) "],["finding-mean-and-standard-deviation.html", "3.11 Finding mean and standard deviation", " 3.11 Finding mean and standard deviation The mean or expected value is the most important measure of central tendency, while the standard deviation (SD, denoted \\(sigma\\) in a population, \\(S(X)\\) in a sample) is a measure of an average dispersion (spread) of [random] variable values about the mean - SD is the most important measure of dispersion. The standard deviation and variance are very important measures of dispersion (of data). The standard deviation is a square root of the variance. The variance of a data set is the arithmetic average of the squared differences between the values and the mean. A low standard deviation indicates that the data points tend to be very close to the mean (also called the expected value); a high standard deviation indicates that the data points are spread out over a large range of values. The standard deviation measures an average dispersion of data (observations) around the mean. The standard deviation is expressed in the same units as the mean (dollars, meters, etc.) is, whereas the variance is expressed in squared units. The variance and standard deviation are very similar measures, but it is easier to interpret standard deviation (is expressed in the same units as the mean). 3.11.1 Example 3.1 3.11.1.1 Example 3.1 (step 1) - raw, unordered data The table below presents an unordered sequence of data about the wages of people (in thousands of dollars) in the two small random samples (variables X and Y). Calculate the mean value (expected value or average) (denoted \\(E(X)\\) or \\(\\mu\\) in a population and \\(\\bar{x}\\) in a sample) of the earnings for each sample and also find the variance and the standard deviation in both samples. Using data from the example, organize data for the variable X into frequency distribution. \\(i\\) \\(x_i\\) \\(y_i\\) 1 1 4 2 3 3 3 4 3 4 5 4 5 3 5 6 4 3 7 6 3 8 5 4 9 4 5 10 2 3 11 1 2 12 3 3 \\(\\Sigma\\) 42 42 Then, we have: \\(i\\) \\(x_i\\) \\(x_i - \\bar{x}\\) \\((x_i - \\bar{x})^2\\) 1 1 -2.5 6.25 2 3 -0.5 0.25 3 4 0.5 0.25 4 5 1.5 2.25 5 3 -0.5 0.25 6 4 0.5 0.25 7 6 2.5 6.25 8 5 1.5 2.25 9 4 0.5 0.25 10 2 -1.5 2.25 11 1 -2.5 6.25 12 3 -0.5 0.25 \\(\\Sigma\\) 42 0 The mean \\(\\bar{x} = 42/12 = 3.5\\), and \\(S(X) = \\sqrt{27/12} = 1.5\\) (thousands of dollars) 3.11.1.2 Example 3.1 (step 2) - data in a frequency distribution Try to repeat the computation of the mean and the standard deviation, but apply the formulas appropriate for the data organized into a frequency distribution. The table presents the frequency distribution for the values of variable X from the table; \\(f_i\\) denotes the absolute frequencies. \\(i\\) \\(x_i\\) \\(f_i\\) 1 1 1 2 2 2 3 3 3 4 4 3 5 5 2 6 6 1 \\(\\Sigma\\) - 12 It depends on the form of data presentation (unordered or ordered distribution), what formula you need to apply to compute the mean and the standard deviation - see the table with formulas from a section above. \\(i\\) \\(x_i\\) \\(f_i\\) \\(x_if_i\\) \\(p_i\\) \\((x_i-\\bar{x})^2\\) \\((x_i-\\bar{x})^2 * p_i\\) \\((x_i-\\bar{x})^2 * f_i\\) 1 1 2 2 2/12 6.25 1.04 12.5 2 2 1 2 1/12 2.25 0.19 2.25 3 3 3 9 3/12 0.25 0.06 0.75 4 4 3 12 3/12 0.25 0.06 0.75 5 5 2 10 2/12 2.25 0.38 4.5 6 6 1 6 1/12 6.25 0.52 6.25 \\(\\Sigma\\) - 12 42 1 - 2.25 27 Thus, the average value is \\(\\bar{x} = 42/12 = 3.5\\), and the average spread of the observations about the mean \\(S(X)\\) is \\(\\sqrt{27/12} = 1.5\\) thousand ($1500). "],["mode.html", "3.12 Mode", " 3.12 Mode A mode is the most commonly occurring value in a data set. Calculate the mode of the numbers: 2, 2, 2, 6, 6, 6. In this case, the modal value (Md) does not exist. "],["quartiles-and-median.html", "3.13 Quartiles and median", " 3.13 Quartiles and median After a data set has been sorted in ascending order (smallest to largest), the rth percentile, is a value such that r percent of the observations in the data set fall at or below that value. For instance: The median, Q2, is the 50th percentile of a data set. For an odd number of data, the median is one of the original data values. For an even number of data, the median is the average of the two middle values, and hence may not be in the original data set. The 25th percentile is also known as the first quartile (Q1), the 50th percentile as the median or second quartile (Q2), and the 75th percentile as the third quartile (Q3). In general, percentiles and quartiles are specific types of quantiles - q-quantiles are values that partition a finite set of values into q subsets of (nearly) equal sizes. After a data set has been sorted in ascending order (smallest to largest), - the median of a data set is its middle value (the 50th percentile, the 2nd quartile). - the lower quartile is the cutoff point for the bottom 25% of the data (the 25th perc.). - the upper quartile is the cutoff point for the top 25% of the data (the 75th per.). 3.13.1 Example A = {2, 4, 4, 5, 6, 7, 8} \\(Q_1 = 4\\) \\(Q_2 (median) = 5\\) \\(Q_3 = 7\\) A = {1, 3, 3, 4, 5 | 6, 6, 7, 8, 8} \\(Q_1 = 3\\) \\(Q_2 (median) = (5+6)/2 = 5.5\\) \\(Q_3 = 7\\) 3.13.1.1 Interquartile-range (IQR) \\(IQR = Q_3 - Q_1\\) Also, we employ IQR to determine outliers; atypical values (outliers) are usualy defined as values of a variable that not fall into the range: \\[[Q_1 - 1.5 \\times IQR, Q_3 + 1.5 \\times IQR]\\] "],["measures-of-relative-standing.html", "3.14 Measures of relative standing", " 3.14 Measures of relative standing 3.14.1 Percentile and percentile rank The percentile system is widely used to show how an individual has performed relative to a known group. It is based on the cumulative percentage distribution. A percentile point is a point on the measurement scale below which a specified percentage of the cases in the distribution falls. It is more commonly called a percentile (from the Latin centrum, meaning “hundred”). If, for example, 50% of students in the history course have midterm scores lower than 81, the 50th percentile is 81. A percentile rank is the percentage of cases falling below a given point on the measurement scale. In our example, the percentile rank of a score of 81 is 50. Do not confuse percentiles and percentile ranks: Percentile ranks may take values only between 0 and 100, whereas a percentile (point) may have any value that scores may have. For example, it is possible that the value of a percentile is 143. Suppose that Mary earned a score of 143 on a college entrance test and that 75% of applicants score lower. The 75th percentile is 143; Mary’s percentile rank is 75. Standardized tests, such as the SAT (Scholastic Aptitude Test) and GRE (Graduate Record Exam), publish their results in terms of either percentiles or percentile ranks. 3.14.2 Standard score (Z-score) In statistics, the standard score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores. It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This process of converting a raw score into a standard score is called standardizing or normalizing. However, “normalizing” can refer to many types of transformations; e.g. feature scaling is used to bring all values into the range [0,1]. This is also called unity-based normalization. This can be generalized to restrict the range of values in the dataset between any arbitrary points a and b. 3.14.2.1 Z-score as a measure of relative standing The z-score for an observation is the number of standard deviations that it falls from the mean. A positive z-score indicates the observation is above the mean. A negative z-score indicates the observation is below the mean. For sample data, the z-score is calculated as: \\[\\text{z-score}_X = \\frac{x_i - \\mu_X}{\\sigma_X} = \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}}\\] The empirical rule tells us that for a bell-shaped distribution, it is unusual for an observation to fall more than 3 standard deviations from the mean. An observation in a bell-shaped distribution is regarded as a potential outlier (an atypical value) if it falls more than 3 standard deviations from the mean. The z-score allows us to tell quickly how surprising or extreme an observation is. The z-score converts an observation (regardless of the observation’s unit of measurement) to a common scale of measurement, which allows comparisons. Also, The z-score is often used in the z-test in standardized testing – the analog of the so-called Student’s t-test (in statistical inference) for a population whose parameters are known, rather than estimated. As it is very unusual to know the entire population, the t-test is much more widely used. "],["basic-methods-of-data-visualization-lecture-4.html", "4 Basic methods of data visualization [Lecture 4]", " 4 Basic methods of data visualization [Lecture 4] bar chart (bar plot) - visualizes the distribution of observations for nominal or ordinal data, histogram (for absolute and relative frequencies and for the so-called densities - only for quantitative data, box plot - for quantitative data, but sometimes can be employed to describe distributions of ordinal data. "],["bar-plot.html", "4.1 Bar plot", " 4.1 Bar plot # Plot a bar plot using R programming language knitr::opts_chunk$set(fig.width=4, fig.height=4) # Knitr settings library(ggplot2) # Load ggplot package data = data.frame( x = c(1,1,5,5,5,5,5,3,3,3,3,4,4,5) ) # Create data barplot &lt;- ggplot(data = data) + geom_bar( aes(x = x), stat = &quot;count&quot; ) # Create a bar plot # Print a bar plot print(barplot) "],["frequency-distribution-table-and-histogram.html", "4.2 Frequency distribution table and histogram", " 4.2 Frequency distribution table and histogram To visualize frequency distribution you can use frequency or density histogram Id Class interval (bin) \\(h_k\\) for \\(x_i\\) Frequency \\(f_i\\) Relative frequency (fraction) \\(p_i = f_i/n\\) Density \\(d_i = p_i/\\Delta x_i\\) 1 [1, 6) 12 12/120 = 0.1 0.1/(6 - 1) = 0.02 2 [6, 11) 6 0.05 0.01 … … … … … 20 [96, 101) 3 0.025 0.005 \\(\\Sigma\\) - \\(n\\) = 120 1 - "],["three-types-of-histograms-are-used.html", "4.3 Three types of histograms are used:", " 4.3 Three types of histograms are used: histogram of absolute rates (to be used only in justified cases); histogram of relative frequencies - the sum of the heights of all bars is equal to 1 (or 100%); density histogram (the so-called true histogram) - the sum of the area of all bars is equal to 1 (or 100%). "],["density-histogram-and-frequency-histogram.html", "4.4 Density histogram and frequency histogram", " 4.4 Density histogram and frequency histogram 4.4.1 Construction of the density histogram (1) # Plot histogram using R knitr::opts_chunk$set(fig.width=4, fig.height=4) # Knitr settings library(lattice) # Load lattice package data = c(140,145,200,325,320,285,283,280,248,246,242,240,240,204,201) # Create data set histogram &lt;- histogram(data, nint = 5, type = &quot;density&quot;, endpoints = c(140, 340), right = F) # Create histogram print(histogram) # Plot histogran histogram$panel.args.common$breaks # Print interval breaks used to plot the histogram ## [1] 140 180 220 260 300 340 4.4.2 Absolute histogram histogram &lt;- histogram(data, nint = 5, type = &quot;count&quot;) # Create histogram print(histogram) # Print histogram 4.4.3 Relative frequency histogram histogram &lt;- histogram(data, nint = 5, type = &quot;percent&quot;) # Create histogram print(histogram) # Print histogram "],["histogram-for-the-binomial-distribution-1.html", "4.5 Histogram for the binomial distribution (1)", " 4.5 Histogram for the binomial distribution (1) library(ggplot2) # Generate a random sample Z &lt;- seq(0,4,by = 1) # Calculate probabilities P &lt;- dbinom(Z,4,0.25) binom_data &lt;- data.frame(Z, P) # Merge data sets head(binom_data) ## Z P ## 1 0 0.31640625 ## 2 1 0.42187500 ## 3 2 0.21093750 ## 4 3 0.04687500 ## 5 4 0.00390625 # Plot data ggplot(data = binom_data) + geom_bar( aes(x = Z, y = P), stat = &quot;identity&quot; ) "],["box-plot.html", "4.6 Box plot", " 4.6 Box plot Like bar plots and histograms, the box plot is used to show the distribution of the data. This type of plot illustrates the basic characteristics of a data distribution, and is often used when researchers want to see how the distribution of a quantitative variable is affected by some qualitative variable (or discrete quantitative) (e.g .: life expectancy vs. continent). "],["box-plot-example-1.html", "4.7 Box plot - example (1)", " 4.7 Box plot - example (1) # Plot box plots using R data &lt;- data.frame( men = c(143, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200), women = c(140, 150, 155, 158, 160, 163, 163, 165, 166, 166, 168, 170, 170, 171, 172, 173, 173, 173, 175, 176, 177, 181, 182, 183, 196) ) # Data set head(data) # Displays the first lines of a dataset ## men women ## 1 143 140 ## 2 160 150 ## 3 165 155 ## 4 168 158 ## 5 172 160 ## 6 173 163 library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidyr) # Load packages data &lt;- gather(data = data, key = &quot;plec&quot;, value = &quot;wzrost&quot;, men:women ) # Transform dataset from wide to long format head(data) ## plec wzrost ## 1 men 143 ## 2 men 160 ## 3 men 165 ## 4 men 168 ## 5 men 172 ## 6 men 173 4.7.1 Box plot - example (2) box &lt;- ggplot(data = data) + geom_boxplot( aes( x = plec, y = wzrost ) ) + coord_flip() # Rysuje wykres pudełkowy print(box) # Wyświetla wykres 4.7.2 Box plot - example (compare the box plots) True or False: Women in the studied sample are, on average, taller than men (F); Male height is more dispersed relative to the median (…); The lowest person is a woman (…); Both data sets negatively skewed (…); Half of the women measure at least 170 cm (…). 4.7.3 Plot the data from the above example using histograms and smooth density curves hist &lt;- ggplot(data = data) + geom_histogram( aes( x = wzrost, y = ..density.., fill = plec ), bins = 6, alpha = 0.7 ) + geom_density( aes( x = wzrost, y = ..density.., colour = plec ) ) + theme_classic() # Create histograms print(hist) # Print the result "],["box-plots-additional-example.html", "4.8 Box plots - additional example", " 4.8 Box plots - additional example X = c(215, 140, 290, 300, 300, 320, 330, 340, 415, 480) # Define the data set boxplot(X, horizontal = T) # Plot the data "],["histogram-and-box-plot.html", "4.9 Histogram and box-plot", " 4.9 Histogram and box-plot "],["skewness-of-data-distribution.html", "4.10 Skewness of data distribution", " 4.10 Skewness of data distribution Also, "]]
